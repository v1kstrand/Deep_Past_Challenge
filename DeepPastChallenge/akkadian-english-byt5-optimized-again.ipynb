{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298cbd34",
   "metadata": {
    "papermill": {
     "duration": 0.005422,
     "end_time": "2026-02-02T12:45:32.920833",
     "exception": false,
     "start_time": "2026-02-02T12:45:32.915411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1Ô∏è‚É£ System-Level Optimization\n",
    "\n",
    "Set optimal environment variables **before** importing PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1afdbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:32.931472Z",
     "iopub.status.busy": "2026-02-02T12:45:32.931117Z",
     "iopub.status.idle": "2026-02-02T12:45:32.941090Z",
     "shell.execute_reply": "2026-02-02T12:45:32.939770Z"
    },
    "papermill": {
     "duration": 0.017869,
     "end_time": "2026-02-02T12:45:32.943193",
     "exception": false,
     "start_time": "2026-02-02T12:45:32.925324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables optimized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Optimize PyTorch/CUDA performance\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "print(\"‚úÖ Environment variables optimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e15b37",
   "metadata": {
    "papermill": {
     "duration": 0.004309,
     "end_time": "2026-02-02T12:45:32.951982",
     "exception": false,
     "start_time": "2026-02-02T12:45:32.947673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2Ô∏è‚É£ Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92029ab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:32.963578Z",
     "iopub.status.busy": "2026-02-02T12:45:32.963238Z",
     "iopub.status.idle": "2026-02-02T12:45:52.005167Z",
     "shell.execute_reply": "2026-02-02T12:45:52.004116Z"
    },
    "papermill": {
     "duration": 19.049818,
     "end_time": "2026-02-02T12:45:52.007075",
     "exception": false,
     "start_time": "2026-02-02T12:45:32.957257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18790659",
   "metadata": {
    "papermill": {
     "duration": 0.004318,
     "end_time": "2026-02-02T12:45:52.015877",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.011559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3Ô∏è‚É£ Configuration\n",
    "\n",
    "**üìù EDIT THESE PATHS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2fd603d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.026620Z",
     "iopub.status.busy": "2026-02-02T12:45:52.026133Z",
     "iopub.status.idle": "2026-02-02T12:45:52.037149Z",
     "shell.execute_reply": "2026-02-02T12:45:52.036331Z"
    },
    "papermill": {
     "duration": 0.019252,
     "end_time": "2026-02-02T12:45:52.039376",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.020124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Configuration:\n",
      "  Device: cpu\n",
      "  Batch size: 8\n",
      "  Beams: 8\n",
      "\n",
      "üöÄ Optimizations:\n",
      "  Mixed Precision: False\n",
      "  BetterTransformer: False\n",
      "  Bucket Batching: True\n",
      "  Vectorized Postproc: True\n",
      "  Adaptive Beams: True\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class UltraConfig:\n",
    "    \"\"\"Ultra-optimized configuration\"\"\"\n",
    "    \n",
    "    # ============ PATHS - EDIT THESE ============\n",
    "    test_data_path: str = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
    "    model_path: str = \"/kaggle/input/final-byt5/byt5-akkadian-optimized-34x\"\n",
    "    output_dir: str = \"/kaggle/working/\"\n",
    "    \n",
    "    # ============ PROCESSING ============\n",
    "    max_length: int = 512\n",
    "    batch_size: int = 8  # Will auto-tune if use_auto_batch_size=True\n",
    "    num_workers: int = 4  # Increased for better throughput\n",
    "    \n",
    "    # ============ GENERATION ============\n",
    "    num_beams: int = 8\n",
    "    max_new_tokens: int = 512\n",
    "    length_penalty: float = 1.5\n",
    "    repetition_penalty: float = 1.2\n",
    "    early_stopping: bool = True\n",
    "    no_repeat_ngram_size: int = 0  # Set to 3 if you see repetition\n",
    "    \n",
    "    # ============ OPTIMIZATIONS ============\n",
    "    use_mixed_precision: bool = True      # FP16 for 2x speedup\n",
    "    use_better_transformer: bool = True   # 20-50% speedup\n",
    "    use_bucket_batching: bool = True      # 20-40% less padding\n",
    "    use_vectorized_postproc: bool = True  # 3-5x faster postproc\n",
    "    use_adaptive_beams: bool = True       # Smart beam allocation\n",
    "    use_auto_batch_size: bool = False     # Auto-find optimal batch size\n",
    "    \n",
    "    # ============ OTHER ============\n",
    "    aggressive_postprocessing: bool = True\n",
    "    checkpoint_freq: int = 100\n",
    "    num_buckets: int = 4  # For bucket batching\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        Path(self.output_dir).mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            self.use_mixed_precision = False\n",
    "            self.use_better_transformer = False\n",
    "\n",
    "# Create config\n",
    "config = UltraConfig()\n",
    "\n",
    "print(\"\\nüìã Configuration:\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Beams: {config.num_beams}\")\n",
    "print(f\"\\nüöÄ Optimizations:\")\n",
    "print(f\"  Mixed Precision: {config.use_mixed_precision}\")\n",
    "print(f\"  BetterTransformer: {config.use_better_transformer}\")\n",
    "print(f\"  Bucket Batching: {config.use_bucket_batching}\")\n",
    "print(f\"  Vectorized Postproc: {config.use_vectorized_postproc}\")\n",
    "print(f\"  Adaptive Beams: {config.use_adaptive_beams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69033a29",
   "metadata": {
    "papermill": {
     "duration": 0.004312,
     "end_time": "2026-02-02T12:45:52.048109",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.043797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4Ô∏è‚É£ Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2646cb1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.058239Z",
     "iopub.status.busy": "2026-02-02T12:45:52.057917Z",
     "iopub.status.idle": "2026-02-02T12:45:52.065195Z",
     "shell.execute_reply": "2026-02-02T12:45:52.064322Z"
    },
    "papermill": {
     "duration": 0.014568,
     "end_time": "2026-02-02T12:45:52.066946",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.052378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 12:45:52,061 - INFO - Logging initialized\n"
     ]
    }
   ],
   "source": [
    "def setup_logging(output_dir: str = './outputs'):\n",
    "    \"\"\"Setup logging\"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "    log_file = Path(output_dir) / 'inference_ultra.log'\n",
    "    \n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler(log_file)\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging(config.output_dir)\n",
    "logger.info(\"Logging initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d1e31",
   "metadata": {
    "papermill": {
     "duration": 0.004322,
     "end_time": "2026-02-02T12:45:52.075705",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.071383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5Ô∏è‚É£ Optimized Text Preprocessor\n",
    "\n",
    "Uses pre-compiled regex patterns for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bc8cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.085846Z",
     "iopub.status.busy": "2026-02-02T12:45:52.085498Z",
     "iopub.status.idle": "2026-02-02T12:45:52.095563Z",
     "shell.execute_reply": "2026-02-02T12:45:52.094692Z"
    },
    "papermill": {
     "duration": 0.01731,
     "end_time": "2026-02-02T12:45:52.097309",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.079999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input:  lugal ... xxx mu.2.kam\n",
      "Preprocessed: lugal <big_gap> <gap> mu.2.kam\n"
     ]
    }
   ],
   "source": [
    "class OptimizedPreprocessor:\n",
    "    \"\"\"Preprocessor with pre-compiled patterns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Pre-compile regex patterns (20-30% faster)\n",
    "        self.patterns = {\n",
    "            'big_gap': re.compile(r'(\\.{3,}|‚Ä¶+|‚Ä¶‚Ä¶)'),\n",
    "            'small_gap': re.compile(r'(xx+|\\s+x\\s+)'),\n",
    "        }\n",
    "    \n",
    "    def preprocess_input_text(self, text: str) -> str:\n",
    "        \"\"\"Single text preprocessing\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        text = self.patterns['big_gap'].sub('<big_gap>', text)\n",
    "        text = self.patterns['small_gap'].sub('<gap>', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"Vectorized batch preprocessing (faster)\"\"\"\n",
    "        s = pd.Series(texts).fillna(\"\")\n",
    "        s = s.astype(str)\n",
    "        s = s.str.replace(self.patterns['big_gap'], '<big_gap>', regex=True)\n",
    "        s = s.str.replace(self.patterns['small_gap'], '<gap>', regex=True)\n",
    "        return s.tolist()\n",
    "\n",
    "# Test\n",
    "preprocessor = OptimizedPreprocessor()\n",
    "test = \"lugal ... xxx mu.2.kam\"\n",
    "print(f\"Test input:  {test}\")\n",
    "print(f\"Preprocessed: {preprocessor.preprocess_input_text(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8df94",
   "metadata": {
    "papermill": {
     "duration": 0.004379,
     "end_time": "2026-02-02T12:45:52.106526",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.102147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6Ô∏è‚É£ Vectorized Postprocessor\n",
    "\n",
    "Uses pandas for batch operations ‚Üí **3-5x faster** than loop-based postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61878d65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.117415Z",
     "iopub.status.busy": "2026-02-02T12:45:52.116668Z",
     "iopub.status.idle": "2026-02-02T12:45:52.142329Z",
     "shell.execute_reply": "2026-02-02T12:45:52.141285Z"
    },
    "papermill": {
     "duration": 0.033459,
     "end_time": "2026-02-02T12:45:52.144319",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.110860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test postprocessing:\n",
      "  The king (plur.) took the city... [x] [x]\n",
      "  ‚Üí The king plur. took the city <big_gap>\n",
      "  He spoke spoke to the assembly\n",
      "  ‚Üí He spoke to the assembly\n"
     ]
    }
   ],
   "source": [
    "class VectorizedPostprocessor:\n",
    "    \"\"\"Ultra-fast vectorized postprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, aggressive: bool = True):\n",
    "        self.aggressive = aggressive\n",
    "        \n",
    "        # Pre-compile ALL patterns\n",
    "        self.patterns = {\n",
    "            'gap': re.compile(r'(\\[x\\]|\\(x\\)|\\bx\\b)', re.I),\n",
    "            'big_gap': re.compile(r'(\\.{3,}|‚Ä¶|\\[\\.+\\])'),\n",
    "            'annotations': re.compile(r'\\((fem|plur|pl|sing|singular|plural|\\?|!)\\..\\s*\\w*\\)', re.I),\n",
    "            'repeated_words': re.compile(r'\\b(\\w+)(?:\\s+\\1\\b)+'),\n",
    "            'whitespace': re.compile(r'\\s+'),\n",
    "            'punct_space': re.compile(r'\\s+([.,:])'),\n",
    "            'repeated_punct': re.compile(r'([.,])\\1+'),\n",
    "        }\n",
    "        \n",
    "        # Character translation tables\n",
    "        self.subscript_trans = str.maketrans(\"‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ\", \"0123456789\")\n",
    "        self.special_chars_trans = str.maketrans('·∏´·∏™', 'hH')\n",
    "        self.forbidden_chars = '!?()\"‚Äî‚Äî<>‚åà‚åã‚åä[]+ æ/;'\n",
    "        self.forbidden_trans = str.maketrans('', '', self.forbidden_chars)\n",
    "    \n",
    "    def postprocess_batch(self, translations: List[str]) -> List[str]:\n",
    "        \"\"\"Vectorized batch postprocessing - 3-5x faster than loop\"\"\"\n",
    "        \n",
    "        # Convert to Series for vectorized operations\n",
    "        s = pd.Series(translations)\n",
    "        \n",
    "        # Filter invalid entries\n",
    "        valid_mask = s.apply(lambda x: isinstance(x, str) and x.strip())\n",
    "        if not valid_mask.all():\n",
    "            s[~valid_mask] = \"\"\n",
    "        \n",
    "        # Basic cleaning (always applied)\n",
    "        s = s.str.translate(self.special_chars_trans)\n",
    "        s = s.str.translate(self.subscript_trans)\n",
    "        s = s.str.replace(self.patterns['whitespace'], ' ', regex=True)\n",
    "        s = s.str.strip()\n",
    "        \n",
    "        if self.aggressive:\n",
    "            # Normalize gaps\n",
    "            s = s.str.replace(self.patterns['gap'], '<gap>', regex=True)\n",
    "            s = s.str.replace(self.patterns['big_gap'], '<big_gap>', regex=True)\n",
    "            \n",
    "            # Merge adjacent gaps\n",
    "            s = s.str.replace('<gap> <gap>', '<big_gap>', regex=False)\n",
    "            s = s.str.replace('<big_gap> <big_gap>', '<big_gap>', regex=False)\n",
    "            \n",
    "            # Remove annotations\n",
    "            s = s.str.replace(self.patterns['annotations'], '', regex=True)\n",
    "            \n",
    "            # Protect gaps during char removal\n",
    "            s = s.str.replace('<gap>', '\\x00GAP\\x00', regex=False)\n",
    "            s = s.str.replace('<big_gap>', '\\x00BIG\\x00', regex=False)\n",
    "            \n",
    "            # Remove forbidden characters\n",
    "            s = s.str.translate(self.forbidden_trans)\n",
    "            \n",
    "            # Restore gaps\n",
    "            s = s.str.replace('\\x00GAP\\x00', ' <gap> ', regex=False)\n",
    "            s = s.str.replace('\\x00BIG\\x00', ' <big_gap> ', regex=False)\n",
    "            \n",
    "            # Fractions (vectorized)\n",
    "            s = s.str.replace(r'(\\d+)\\.5\\b', r'\\1¬Ω', regex=True)\n",
    "            s = s.str.replace(r'\\b0\\.5\\b', '¬Ω', regex=True)\n",
    "            s = s.str.replace(r'(\\d+)\\.25\\b', r'\\1¬º', regex=True)\n",
    "            s = s.str.replace(r'\\b0\\.25\\b', '¬º', regex=True)\n",
    "            s = s.str.replace(r'(\\d+)\\.75\\b', r'\\1¬æ', regex=True)\n",
    "            s = s.str.replace(r'\\b0\\.75\\b', '¬æ', regex=True)\n",
    "            \n",
    "            # Remove repeated words\n",
    "            s = s.str.replace(self.patterns['repeated_words'], r'\\1', regex=True)\n",
    "            \n",
    "            # Remove repeated n-grams\n",
    "            for n in range(4, 1, -1):\n",
    "                pattern = r'\\b((?:\\w+\\s+){' + str(n-1) + r'}\\w+)(?:\\s+\\1\\b)+'\n",
    "                s = s.str.replace(pattern, r'\\1', regex=True)\n",
    "            \n",
    "            # Fix punctuation\n",
    "            s = s.str.replace(self.patterns['punct_space'], r'\\1', regex=True)\n",
    "            s = s.str.replace(self.patterns['repeated_punct'], r'\\1', regex=True)\n",
    "            \n",
    "            # Final cleanup\n",
    "            s = s.str.replace(self.patterns['whitespace'], ' ', regex=True)\n",
    "            s = s.str.strip().str.strip('-').str.strip()\n",
    "        \n",
    "        return s.tolist()\n",
    "\n",
    "# Test\n",
    "postprocessor = VectorizedPostprocessor(aggressive=config.aggressive_postprocessing)\n",
    "test_outputs = [\n",
    "    \"The king (plur.) took the city... [x] [x]\",\n",
    "    \"He spoke spoke to the assembly\"\n",
    "]\n",
    "cleaned = postprocessor.postprocess_batch(test_outputs)\n",
    "print(\"Test postprocessing:\")\n",
    "for orig, clean in zip(test_outputs, cleaned):\n",
    "    print(f\"  {orig}\")\n",
    "    print(f\"  ‚Üí {clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f52703",
   "metadata": {
    "papermill": {
     "duration": 0.004447,
     "end_time": "2026-02-02T12:45:52.153971",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.149524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7Ô∏è‚É£ Bucket Batch Sampler\n",
    "\n",
    "Groups samples by length to minimize padding ‚Üí **20-40% faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a11ed0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.164979Z",
     "iopub.status.busy": "2026-02-02T12:45:52.164598Z",
     "iopub.status.idle": "2026-02-02T12:45:52.174109Z",
     "shell.execute_reply": "2026-02-02T12:45:52.172918Z"
    },
    "papermill": {
     "duration": 0.017368,
     "end_time": "2026-02-02T12:45:52.175838",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.158470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BucketBatchSampler(Sampler):\n",
    "    \"\"\"Batch samples by similar length to minimize padding\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size: int, num_buckets: int = 4, shuffle: bool = False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Calculate lengths\n",
    "        lengths = [len(text.split()) for _, text in dataset]\n",
    "        \n",
    "        # Sort indices by length\n",
    "        sorted_indices = sorted(range(len(lengths)), key=lambda i: lengths[i])\n",
    "        \n",
    "        # Create buckets\n",
    "        bucket_size = len(sorted_indices) // num_buckets\n",
    "        self.buckets = []\n",
    "        for i in range(num_buckets):\n",
    "            start = i * bucket_size\n",
    "            end = None if i == num_buckets - 1 else (i + 1) * bucket_size\n",
    "            self.buckets.append(sorted_indices[start:end])\n",
    "        \n",
    "        # Log bucket info\n",
    "        logger.info(f\"Created {num_buckets} buckets:\")\n",
    "        for i, bucket in enumerate(self.buckets):\n",
    "            bucket_lengths = [lengths[idx] for idx in bucket]\n",
    "            logger.info(f\"  Bucket {i}: {len(bucket)} samples, \"\n",
    "                       f\"length range [{min(bucket_lengths)}, {max(bucket_lengths)}]\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for bucket in self.buckets:\n",
    "            if self.shuffle:\n",
    "                random.shuffle(bucket)\n",
    "            \n",
    "            for i in range(0, len(bucket), self.batch_size):\n",
    "                yield bucket[i:i+self.batch_size]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum((len(b) + self.batch_size - 1) // self.batch_size for b in self.buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08077442",
   "metadata": {
    "papermill": {
     "duration": 0.004612,
     "end_time": "2026-02-02T12:45:52.184996",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.180384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8Ô∏è‚É£ Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d9a06e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.196466Z",
     "iopub.status.busy": "2026-02-02T12:45:52.195851Z",
     "iopub.status.idle": "2026-02-02T12:45:52.201943Z",
     "shell.execute_reply": "2026-02-02T12:45:52.201054Z"
    },
    "papermill": {
     "duration": 0.013744,
     "end_time": "2026-02-02T12:45:52.203682",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.189938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AkkadianDataset(Dataset):\n",
    "    \"\"\"Optimized dataset with batch preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe: pd.DataFrame, preprocessor: OptimizedPreprocessor):\n",
    "        self.sample_ids = dataframe['id'].tolist()\n",
    "        \n",
    "        # Batch preprocess (faster than loop)\n",
    "        raw_texts = dataframe['transliteration'].tolist()\n",
    "        preprocessed = preprocessor.preprocess_batch(raw_texts)\n",
    "        \n",
    "        # Add task prefix\n",
    "        self.input_texts = [\n",
    "            \"translate Akkadian to English: \" + text\n",
    "            for text in preprocessed\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"Dataset created with {len(self.sample_ids)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.sample_ids[index], self.input_texts[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c1d07",
   "metadata": {
    "papermill": {
     "duration": 0.004934,
     "end_time": "2026-02-02T12:45:52.213147",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.208213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9Ô∏è‚É£ Ultra-Optimized Inference Engine\n",
    "\n",
    "Main inference engine with all optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68796311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.225506Z",
     "iopub.status.busy": "2026-02-02T12:45:52.225012Z",
     "iopub.status.idle": "2026-02-02T12:45:52.256945Z",
     "shell.execute_reply": "2026-02-02T12:45:52.256141Z"
    },
    "papermill": {
     "duration": 0.040692,
     "end_time": "2026-02-02T12:45:52.258710",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.218018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference engine defined\n"
     ]
    }
   ],
   "source": [
    "class UltraInferenceEngine:\n",
    "    \"\"\"Ultra-optimized inference engine\"\"\"\n",
    "    \n",
    "    def __init__(self, config: UltraConfig):\n",
    "        self.config = config\n",
    "        self.preprocessor = OptimizedPreprocessor()\n",
    "        self.postprocessor = VectorizedPostprocessor(aggressive=config.aggressive_postprocessing)\n",
    "        self.results = []\n",
    "        \n",
    "        # Load model\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load and optimize model\"\"\"\n",
    "        logger.info(f\"Loading model from {self.config.model_path}\")\n",
    "        \n",
    "        try:\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                self.config.model_path\n",
    "            ).to(self.config.device).eval()\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)\n",
    "            \n",
    "            num_params = sum(p.numel() for p in self.model.parameters())\n",
    "            logger.info(f\"Model loaded: {num_params:,} parameters\")\n",
    "            \n",
    "            # Apply BetterTransformer\n",
    "            if self.config.use_better_transformer and torch.cuda.is_available():\n",
    "                try:\n",
    "                    from optimum.bettertransformer import BetterTransformer\n",
    "                    logger.info(\"Applying BetterTransformer...\")\n",
    "                    self.model = BetterTransformer.transform(self.model)\n",
    "                    logger.info(\"‚úÖ BetterTransformer applied (20-50% speedup)\")\n",
    "                except ImportError:\n",
    "                    logger.warning(\"‚ö†Ô∏è  'optimum' not installed, skipping BetterTransformer\")\n",
    "                    logger.warning(\"   Install with: !pip install optimum\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è  BetterTransformer failed: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _collate_fn(self, batch_samples):\n",
    "        \"\"\"Collate function\"\"\"\n",
    "        batch_ids = [s[0] for s in batch_samples]\n",
    "        batch_texts = [s[1] for s in batch_samples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=self.config.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return batch_ids, tokenized\n",
    "    \n",
    "    def find_optimal_batch_size(self, dataset, start_bs: int = 32):\n",
    "        \"\"\"Binary search for optimal batch size\"\"\"\n",
    "        logger.info(\"üîç Finding optimal batch size...\")\n",
    "        \n",
    "        max_bs = start_bs\n",
    "        min_bs = 1\n",
    "        \n",
    "        while max_bs - min_bs > 1:\n",
    "            test_bs = (max_bs + min_bs) // 2\n",
    "            \n",
    "            try:\n",
    "                test_batch = [dataset[i] for i in range(min(test_bs, len(dataset)))]\n",
    "                ids, inputs = self._collate_fn(test_batch)\n",
    "                \n",
    "                with torch.inference_mode():\n",
    "                    if self.config.use_mixed_precision:\n",
    "                        with autocast():\n",
    "                            outputs = self.model.generate(\n",
    "                                input_ids=inputs.input_ids.to(self.config.device),\n",
    "                                attention_mask=inputs.attention_mask.to(self.config.device),\n",
    "                                num_beams=self.config.num_beams,\n",
    "                                max_new_tokens=64,\n",
    "                                use_cache=True\n",
    "                            )\n",
    "                    else:\n",
    "                        outputs = self.model.generate(\n",
    "                            input_ids=inputs.input_ids.to(self.config.device),\n",
    "                            attention_mask=inputs.attention_mask.to(self.config.device),\n",
    "                            num_beams=self.config.num_beams,\n",
    "                            max_new_tokens=64,\n",
    "                            use_cache=True\n",
    "                        )\n",
    "                \n",
    "                min_bs = test_bs\n",
    "                logger.info(f\"  ‚úÖ Batch size {test_bs} works\")\n",
    "                \n",
    "                del outputs, inputs\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    max_bs = test_bs\n",
    "                    logger.info(f\"  ‚ùå Batch size {test_bs} OOM\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        optimal = min_bs\n",
    "        logger.info(f\"üéØ Optimal batch size: {optimal}\")\n",
    "        return optimal\n",
    "    \n",
    "    def _get_adaptive_beam_size(self, input_ids, attention_mask):\n",
    "        \"\"\"Adaptive beam size based on complexity\"\"\"\n",
    "        if not self.config.use_adaptive_beams:\n",
    "            return self.config.num_beams\n",
    "        \n",
    "        lengths = attention_mask.sum(dim=1)\n",
    "        \n",
    "        # Short ‚Üí fewer beams, Long ‚Üí more beams\n",
    "        beam_sizes = torch.where(\n",
    "            lengths < 100,\n",
    "            torch.tensor(max(4, self.config.num_beams // 2)),\n",
    "            torch.tensor(self.config.num_beams)\n",
    "        )\n",
    "        \n",
    "        return beam_sizes[0].item()\n",
    "    \n",
    "    def _save_checkpoint(self):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        if len(self.results) > 0 and len(self.results) % self.config.checkpoint_freq == 0:\n",
    "            path = Path(self.config.output_dir) / f\"checkpoint_{len(self.results)}.csv\"\n",
    "            df = pd.DataFrame(self.results, columns=['id', 'translation'])\n",
    "            df.to_csv(path, index=False)\n",
    "            logger.info(f\"üíæ Checkpoint: {len(self.results)} translations\")\n",
    "    \n",
    "    def run_inference(self, test_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run ultra-optimized inference\"\"\"\n",
    "        logger.info(\"üöÄ Starting ULTRA-OPTIMIZED inference\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = AkkadianDataset(test_df, self.preprocessor)\n",
    "        \n",
    "        # Auto-find batch size\n",
    "        if self.config.use_auto_batch_size:\n",
    "            optimal_bs = self.find_optimal_batch_size(dataset)\n",
    "            self.config.batch_size = optimal_bs\n",
    "        \n",
    "        # Create dataloader\n",
    "        if self.config.use_bucket_batching:\n",
    "            batch_sampler = BucketBatchSampler(\n",
    "                dataset, \n",
    "                self.config.batch_size,\n",
    "                num_buckets=self.config.num_buckets\n",
    "            )\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_sampler=batch_sampler,\n",
    "                num_workers=self.config.num_workers,\n",
    "                collate_fn=self._collate_fn,\n",
    "                pin_memory=True,\n",
    "                prefetch_factor=2,\n",
    "                persistent_workers=True if self.config.num_workers > 0 else False\n",
    "            )\n",
    "        else:\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=self.config.num_workers,\n",
    "                collate_fn=self._collate_fn,\n",
    "                pin_memory=True,\n",
    "                prefetch_factor=2,\n",
    "                persistent_workers=True if self.config.num_workers > 0 else False\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"DataLoader created: {len(dataloader)} batches\")\n",
    "        logger.info(f\"Active optimizations:\")\n",
    "        logger.info(f\"  ‚úÖ Mixed Precision: {self.config.use_mixed_precision}\")\n",
    "        logger.info(f\"  ‚úÖ BetterTransformer: {self.config.use_better_transformer}\")\n",
    "        logger.info(f\"  ‚úÖ Bucket Batching: {self.config.use_bucket_batching}\")\n",
    "        logger.info(f\"  ‚úÖ Vectorized Postproc: {self.config.use_vectorized_postproc}\")\n",
    "        logger.info(f\"  ‚úÖ Adaptive Beams: {self.config.use_adaptive_beams}\")\n",
    "        \n",
    "        # Generation config\n",
    "        # Generation config\n",
    "        base_gen_config = {\n",
    "            \"max_new_tokens\": self.config.max_new_tokens,\n",
    "            \"length_penalty\": self.config.length_penalty,\n",
    "            \"repetition_penalty\": self.config.repetition_penalty,  # ADD THIS LINE\n",
    "            \"early_stopping\": self.config.early_stopping,\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "        if self.config.no_repeat_ngram_size > 0:\n",
    "            base_gen_config[\"no_repeat_ngram_size\"] = self.config.no_repeat_ngram_size\n",
    "        \n",
    "        # Run inference\n",
    "        self.results = []\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for batch_idx, (batch_ids, tokenized) in enumerate(tqdm(dataloader, desc=\"üöÄ Translating\")):\n",
    "                try:\n",
    "                    input_ids = tokenized.input_ids.to(self.config.device)\n",
    "                    attention_mask = tokenized.attention_mask.to(self.config.device)\n",
    "                    \n",
    "                    # Adaptive beam size\n",
    "                    beam_size = self._get_adaptive_beam_size(input_ids, attention_mask)\n",
    "                    gen_config = {**base_gen_config, \"num_beams\": beam_size}\n",
    "                    \n",
    "                    # Generate\n",
    "                    if self.config.use_mixed_precision:\n",
    "                        with autocast():\n",
    "                            outputs = self.model.generate(\n",
    "                                input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                **gen_config\n",
    "                            )\n",
    "                    else:\n",
    "                        outputs = self.model.generate(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            **gen_config\n",
    "                        )\n",
    "                    \n",
    "                    # Decode\n",
    "                    translations = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                    \n",
    "                    # Postprocess (vectorized)\n",
    "                    if self.config.use_vectorized_postproc:\n",
    "                        cleaned = self.postprocessor.postprocess_batch(translations)\n",
    "                    else:\n",
    "                        # Fallback to single processing\n",
    "                        cleaned = [self.postprocessor.postprocess_batch([t])[0] for t in translations]\n",
    "                    \n",
    "                    # Store\n",
    "                    self.results.extend(zip(batch_ids, cleaned))\n",
    "                    \n",
    "                    # Checkpoint\n",
    "                    self._save_checkpoint()\n",
    "                    \n",
    "                    # Memory cleanup\n",
    "                    if torch.cuda.is_available() and batch_idx % 10 == 0:\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå Batch {batch_idx} error: {e}\")\n",
    "                    self.results.extend([(bid, \"\") for bid in batch_ids])\n",
    "                    continue\n",
    "        \n",
    "        logger.info(\"‚úÖ Inference completed\")\n",
    "        \n",
    "        # Create results\n",
    "        results_df = pd.DataFrame(self.results, columns=['id', 'translation'])\n",
    "        self._validate_results(results_df)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def _validate_results(self, df: pd.DataFrame):\n",
    "        \"\"\"Validation report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä VALIDATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        empty = df['translation'].str.strip().eq('').sum()\n",
    "        print(f\"\\nEmpty: {empty} ({empty/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        lengths = df['translation'].str.len()\n",
    "        print(f\"\\nüìè Length stats:\")\n",
    "        print(f\"   Mean: {lengths.mean():.1f}, Median: {lengths.median():.1f}\")\n",
    "        print(f\"   Min: {lengths.min()}, Max: {lengths.max()}\")\n",
    "        \n",
    "        short = ((lengths < 5) & (lengths > 0)).sum()\n",
    "        if short > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  {short} very short translations\")\n",
    "        \n",
    "        print(f\"\\nüìù Sample translations:\")\n",
    "        for idx in [0, len(df)//2, -1]:\n",
    "            s = df.iloc[idx]\n",
    "            preview = s['translation'][:70] + \"...\" if len(s['translation']) > 70 else s['translation']\n",
    "            print(f\"   ID {s['id']:4d}: {preview}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Inference engine defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f5ba9",
   "metadata": {
    "papermill": {
     "duration": 0.004496,
     "end_time": "2026-02-02T12:45:52.267819",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.263323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üîü Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd5e49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.279271Z",
     "iopub.status.busy": "2026-02-02T12:45:52.278679Z",
     "iopub.status.idle": "2026-02-02T12:45:52.312521Z",
     "shell.execute_reply": "2026-02-02T12:45:52.311390Z"
    },
    "papermill": {
     "duration": 0.041696,
     "end_time": "2026-02-02T12:45:52.314527",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.272831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 12:45:52,280 - INFO - Loading test data from /kaggle/input/deep-past-initiative-machine-translation/test.csv\n",
      "2026-02-02 12:45:52,296 - INFO - ‚úÖ Loaded 4 test samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 samples:\n",
      "   id   text_id  line_start  line_end  \\\n",
      "0   0  332fda50           1         7   \n",
      "1   1  332fda50           7        14   \n",
      "2   2  332fda50          14        24   \n",
      "3   3  332fda50          25        30   \n",
      "\n",
      "                                     transliteration  \n",
      "0  um-ma k√†-ru-um k√†-ni-ia-ma a-na aa-q√≠-il‚Ä¶ da-t...  \n",
      "1  i-na mup-p√¨-im aa a-lim(ki) ia-t√π u‚Äû-m√¨-im a-n...  \n",
      "2  ki-ma mup-p√¨-ni ta-√°a-me-a-ni a-ma-kam lu a-na...  \n",
      "3  me-+e-er mup-p√¨-ni a-na k√†-ar k√†-ar-ma √∫ wa-ba...  \n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading test data from {config.test_data_path}\")\n",
    "\n",
    "test_df = pd.read_csv(config.test_data_path, encoding='utf-8')\n",
    "logger.info(f\"‚úÖ Loaded {len(test_df)} test samples\")\n",
    "\n",
    "print(\"\\nFirst 5 samples:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527a05a",
   "metadata": {
    "papermill": {
     "duration": 0.004573,
     "end_time": "2026-02-02T12:45:52.323885",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.319312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Run Ultra-Optimized Inference\n",
    "\n",
    "**This is the main cell - all optimizations are active!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516ef78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:45:52.334671Z",
     "iopub.status.busy": "2026-02-02T12:45:52.334349Z",
     "iopub.status.idle": "2026-02-02T12:48:00.397033Z",
     "shell.execute_reply": "2026-02-02T12:48:00.395839Z"
    },
    "papermill": {
     "duration": 128.07371,
     "end_time": "2026-02-02T12:48:00.402192",
     "exception": false,
     "start_time": "2026-02-02T12:45:52.328482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 12:45:52,335 - INFO - Loading model from /kaggle/input/final-byt5/byt5-akkadian-optimized-34x\n",
      "2026-02-02 12:45:55.614879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770036355.859199      17 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770036355.932100      17 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770036356.519880      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770036356.519934      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770036356.519937      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770036356.519940      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-02 12:46:14,056 - INFO - Model loaded: 581,653,248 parameters\n",
      "2026-02-02 12:46:14,058 - INFO - üöÄ Starting ULTRA-OPTIMIZED inference\n",
      "2026-02-02 12:46:14,061 - INFO - Dataset created with 4 samples\n",
      "2026-02-02 12:46:14,062 - INFO - Created 4 buckets:\n",
      "2026-02-02 12:46:14,063 - INFO -   Bucket 0: 1 samples, length range [20, 20]\n",
      "2026-02-02 12:46:14,064 - INFO -   Bucket 1: 1 samples, length range [20, 20]\n",
      "2026-02-02 12:46:14,064 - INFO -   Bucket 2: 1 samples, length range [23, 23]\n",
      "2026-02-02 12:46:14,065 - INFO -   Bucket 3: 1 samples, length range [38, 38]\n",
      "2026-02-02 12:46:14,066 - INFO - DataLoader created: 4 batches\n",
      "2026-02-02 12:46:14,067 - INFO - Active optimizations:\n",
      "2026-02-02 12:46:14,068 - INFO -   ‚úÖ Mixed Precision: False\n",
      "2026-02-02 12:46:14,068 - INFO -   ‚úÖ BetterTransformer: False\n",
      "2026-02-02 12:46:14,069 - INFO -   ‚úÖ Bucket Batching: True\n",
      "2026-02-02 12:46:14,070 - INFO -   ‚úÖ Vectorized Postproc: True\n",
      "2026-02-02 12:46:14,071 - INFO -   ‚úÖ Adaptive Beams: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e51f093f91540b18a86155410380972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üöÄ Translating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 12:48:00,384 - INFO - ‚úÖ Inference completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä VALIDATION REPORT\n",
      "============================================================\n",
      "\n",
      "Empty: 0 (0.00%)\n",
      "\n",
      "üìè Length stats:\n",
      "   Mean: 154.0, Median: 148.0\n",
      "   Min: 71, Max: 249\n",
      "\n",
      "üìù Sample translations:\n",
      "   ID    0: Thus says the Kanesh colony: Speak to our messengers, every single day...\n",
      "   ID    1: In the tablet of the City, you wrote to me in the tablet of the City. ...\n",
      "   ID    2: As soon as you hear our letter, there he has given either for anything...\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create engine\n",
    "engine = UltraInferenceEngine(config)\n",
    "\n",
    "# Run inference\n",
    "results_df = engine.run_inference(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed37980",
   "metadata": {
    "papermill": {
     "duration": 0.005609,
     "end_time": "2026-02-02T12:48:00.413769",
     "exception": false,
     "start_time": "2026-02-02T12:48:00.408160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba2d151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:48:00.430656Z",
     "iopub.status.busy": "2026-02-02T12:48:00.428927Z",
     "iopub.status.idle": "2026-02-02T12:48:00.451038Z",
     "shell.execute_reply": "2026-02-02T12:48:00.450016Z"
    },
    "papermill": {
     "duration": 0.032332,
     "end_time": "2026-02-02T12:48:00.452894",
     "exception": false,
     "start_time": "2026-02-02T12:48:00.420562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 12:48:00,445 - INFO - \n",
      "‚úÖ Submission saved to /kaggle/working/submission.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ ULTRA-OPTIMIZED INFERENCE COMPLETE!\n",
      "============================================================\n",
      "Submission file: /kaggle/working/submission.csv\n",
      "Config file: /kaggle/working/ultra_config.json\n",
      "Log file: /kaggle/working/inference_ultra.log\n",
      "Total translations: 4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save submission\n",
    "output_path = Path(config.output_dir) / 'submission.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "logger.info(f\"\\n‚úÖ Submission saved to {output_path}\")\n",
    "\n",
    "# Save config\n",
    "config_dict = {\n",
    "    \"batch_size\": config.batch_size,\n",
    "    \"num_beams\": config.num_beams,\n",
    "    \"length_penalty\": config.length_penalty,\n",
    "    \"no_repeat_ngram_size\": config.no_repeat_ngram_size,\n",
    "    \"optimizations\": {\n",
    "        \"mixed_precision\": config.use_mixed_precision,\n",
    "        \"better_transformer\": config.use_better_transformer,\n",
    "        \"bucket_batching\": config.use_bucket_batching,\n",
    "        \"vectorized_postproc\": config.use_vectorized_postproc,\n",
    "        \"adaptive_beams\": config.use_adaptive_beams,\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = Path(config.output_dir) / 'ultra_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ULTRA-OPTIMIZED INFERENCE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Submission file: {output_path}\")\n",
    "print(f\"Config file: {config_path}\")\n",
    "print(f\"Log file: {Path(config.output_dir) / 'inference_ultra.log'}\")\n",
    "print(f\"Total translations: {len(results_df)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0cd8c",
   "metadata": {
    "papermill": {
     "duration": 0.005837,
     "end_time": "2026-02-02T12:48:00.465067",
     "exception": false,
     "start_time": "2026-02-02T12:48:00.459230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ [Optional] Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94223b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:48:00.478634Z",
     "iopub.status.busy": "2026-02-02T12:48:00.478268Z",
     "iopub.status.idle": "2026-02-02T12:48:00.500709Z",
     "shell.execute_reply": "2026-02-02T12:48:00.499547Z"
    },
    "papermill": {
     "duration": 0.032188,
     "end_time": "2026-02-02T12:48:00.503106",
     "exception": false,
     "start_time": "2026-02-02T12:48:00.470918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (4, 2)\n",
      "\n",
      "First 10 translations:\n",
      "   id                                        translation\n",
      "0   0  Thus says the Kanesh colony: Speak to our mess...\n",
      "1   3  I sent our tablet to every single place and th...\n",
      "2   1  In the tablet of the City, you wrote to me in ...\n",
      "3   2  As soon as you hear our letter, there he has g...\n",
      "\n",
      "Last 10 translations:\n",
      "   id                                        translation\n",
      "0   0  Thus says the Kanesh colony: Speak to our mess...\n",
      "1   3  I sent our tablet to every single place and th...\n",
      "2   1  In the tablet of the City, you wrote to me in ...\n",
      "3   2  As soon as you hear our letter, there he has g...\n",
      "\n",
      "Length distribution:\n",
      "count      4.000000\n",
      "mean     154.000000\n",
      "std       82.093443\n",
      "min       71.000000\n",
      "25%       94.250000\n",
      "50%      148.000000\n",
      "75%      207.750000\n",
      "max      249.000000\n",
      "Name: translation, dtype: float64\n",
      "\n",
      "Empty translations: 0\n"
     ]
    }
   ],
   "source": [
    "# Load submission\n",
    "submission = pd.read_csv(output_path)\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\nFirst 10 translations:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(f\"\\nLast 10 translations:\")\n",
    "print(submission.tail(10))\n",
    "\n",
    "# Statistics\n",
    "lengths = submission['translation'].str.len()\n",
    "print(f\"\\nLength distribution:\")\n",
    "print(lengths.describe())\n",
    "\n",
    "# Check for issues\n",
    "empty = submission['translation'].str.strip().eq('').sum()\n",
    "print(f\"\\nEmpty translations: {empty}\")\n",
    "\n",
    "if empty > 0:\n",
    "    print(\"\\nEmpty translation IDs:\")\n",
    "    print(submission[submission['translation'].str.strip().eq('')]['id'].tolist())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9366631,
     "sourceId": 14661932,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 153.762841,
   "end_time": "2026-02-02T12:48:03.229781",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-02T12:45:29.466940",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "23d1b342b8ab40b39127d9b746620c97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "624ddf14eaa74d48b1ebf0628b08865d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cdda23a417d544d0beba9a7066f516c3",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_23d1b342b8ab40b39127d9b746620c97",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "7c9b57e63b6f46939f5b059f28aa28de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82f9e33c4e444b9fb8fb962420239f6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f607b2409e2c415b8644bfc9aa011a3e",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_fa42e5bda2ea4a41a501033371f5df8f",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá4/4‚Äá[01:46&lt;00:00,‚Äá25.65s/it]"
      }
     },
     "8a2461ed20f54da19f2fdbf07954ad39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7c9b57e63b6f46939f5b059f28aa28de",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_9d4039a937c9491fadcc9e7024f54cc6",
       "tabbable": null,
       "tooltip": null,
       "value": "üöÄ‚ÄáTranslating:‚Äá100%"
      }
     },
     "9300432e13ec4d828ff61cd2702bcf52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d4039a937c9491fadcc9e7024f54cc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9e51f093f91540b18a86155410380972": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8a2461ed20f54da19f2fdbf07954ad39",
        "IPY_MODEL_624ddf14eaa74d48b1ebf0628b08865d",
        "IPY_MODEL_82f9e33c4e444b9fb8fb962420239f6a"
       ],
       "layout": "IPY_MODEL_9300432e13ec4d828ff61cd2702bcf52",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cdda23a417d544d0beba9a7066f516c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f607b2409e2c415b8644bfc9aa011a3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa42e5bda2ea4a41a501033371f5df8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
