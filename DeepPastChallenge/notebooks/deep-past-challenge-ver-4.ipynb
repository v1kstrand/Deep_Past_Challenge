{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086f079e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T10:58:39.386786Z",
     "iopub.status.busy": "2026-02-01T10:58:39.386406Z",
     "iopub.status.idle": "2026-02-01T10:59:33.529456Z",
     "shell.execute_reply": "2026-02-01T10:59:33.528709Z"
    },
    "papermill": {
     "duration": 54.147633,
     "end_time": "2026-02-01T10:59:33.530892",
     "exception": false,
     "start_time": "2026-02-01T10:58:39.383259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå BetterTransformer NOT available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 10:58:53,968 - INFO - Loading test data: /kaggle/input/deep-past-initiative-machine-translation/test.csv\n",
      "2026-02-01 10:58:53,982 - INFO - ‚úÖ Loaded 4 samples\n",
      "2026-02-01 10:58:53,983 - INFO - Loading model: /kaggle/input/final-byt5/byt5-akkadian-optimized-34x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu126 | CUDA: True\n",
      "GPU: Tesla T4 | 15.6GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 10:58:56.440110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769943536.651258      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769943536.710689      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769943537.209247      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769943537.209292      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769943537.209295      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769943537.209303      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-01 10:59:28,395 - INFO - Model: 581,653,248 params\n",
      "2026-02-01 10:59:28,397 - WARNING - ‚ö†Ô∏è BetterTransformer skipped: No module named 'optimum'\n",
      "2026-02-01 10:59:28,398 - INFO - üöÄ Starting inference\n",
      "2026-02-01 10:59:28,402 - INFO - Dataset: 4 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa68b129f66474799c5d11fe31d0039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üöÄ Translating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 10:59:33,512 - INFO - ‚úÖ Inference complete\n",
      "2026-02-01 10:59:33,520 - INFO - ‚úÖ Saved: /kaggle/working/submission.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä SUBMISSION SUMMARY\n",
      "============================================================\n",
      "Total rows: 4\n",
      "Empty translations: 0\n",
      "Length range: [70, 242]\n",
      "\n",
      "First 3:\n",
      " id                                                                                                                                                                                                                                        translation\n",
      "  0                                                                                                                                                                             Thus says the Kanesh colony: Speak to our messengers every single day.\n",
      "  1                                                             As for the tablet of the City, you wrote to me in the tablet of the City. On this day, whoever informs me or does not detain me in the proceedings, the colony of Kanesh will receive.\n",
      "  2 As soon as you hear our letter, there, either he gave anything to the palace, or he did not give anything until us, or he did not give anything until us. Let us write to me about how much gold is in the letter, and I shall send our messenger.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# AKKADIAN TRANSLATION - ULTRA-OPTIMIZED INFERENCE\n",
    "# Output: submission.csv with columns [id, translation]\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    from optimum.bettertransformer import BetterTransformer\n",
    "    print(\"‚úÖ BetterTransformer available!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå BetterTransformer NOT available\")\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__} | CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} | {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class UltraConfig:\n",
    "    # Paths\n",
    "    test_data_path: str = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
    "    model_path: str = \"/kaggle/input/final-byt5/byt5-akkadian-optimized-34x\"\n",
    "    output_dir: str = \"/kaggle/working/\"\n",
    "    \n",
    "    # Processing\n",
    "    max_length: int = 512\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 4\n",
    "    \n",
    "    # Generation (tuned parameters - do not change)\n",
    "    num_beams: int = 8\n",
    "    max_new_tokens: int = 512\n",
    "    length_penalty: float = 1.3\n",
    "    early_stopping: bool = True\n",
    "    no_repeat_ngram_size: int = 0\n",
    "    \n",
    "    # Optimizations\n",
    "    use_mixed_precision: bool = True\n",
    "    use_better_transformer: bool = True\n",
    "    use_bucket_batching: bool = True\n",
    "    use_vectorized_postproc: bool = True\n",
    "    use_adaptive_beams: bool = True\n",
    "    use_auto_batch_size: bool = False\n",
    "    \n",
    "    tiny_dataset_threshold: int = 64\n",
    "    num_buckets: int = 4\n",
    "    aggressive_postprocessing: bool = True\n",
    "    checkpoint_freq: int = 100\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        Path(self.output_dir).mkdir(exist_ok=True, parents=True)\n",
    "        if not torch.cuda.is_available():\n",
    "            self.use_mixed_precision = False\n",
    "            self.use_better_transformer = False\n",
    "\n",
    "config = UltraConfig()\n",
    "\n",
    "# ============================================================\n",
    "# LOGGING\n",
    "# ============================================================\n",
    "def setup_logging(output_dir: str):\n",
    "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "    for h in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(h)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.StreamHandler(), logging.FileHandler(Path(output_dir)/'inference.log')]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging(config.output_dir)\n",
    "\n",
    "# ============================================================\n",
    "# PREPROCESSOR\n",
    "# ============================================================\n",
    "class OptimizedPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'big_gap': re.compile(r'(\\.{3,}|‚Ä¶+|‚Ä¶‚Ä¶)'),\n",
    "            'small_gap': re.compile(r'(xx+|\\s+x\\s+)'),\n",
    "        }\n",
    "    \n",
    "    def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "        s = pd.Series(texts).fillna(\"\").astype(str)\n",
    "        s = s.str.replace(self.patterns['big_gap'], '<big_gap>', regex=True)\n",
    "        s = s.str.replace(self.patterns['small_gap'], '<gap>', regex=True)\n",
    "        return s.tolist()\n",
    "\n",
    "# ============================================================\n",
    "# POSTPROCESSOR\n",
    "# ============================================================\n",
    "class VectorizedPostprocessor:\n",
    "    def __init__(self, aggressive: bool = True):\n",
    "        self.aggressive = aggressive\n",
    "        self.patterns = {\n",
    "            'gap': re.compile(r'(\\[x\\]|\\(x\\)|\\bx\\b)', re.I),\n",
    "            'big_gap': re.compile(r'(\\.{3,}|‚Ä¶|\\[\\.+\\])'),\n",
    "            'annotations': re.compile(r'\\((fem|plur|pl|sing|singular|plural|\\?|!)\\..\\s*\\w*\\)', re.I),\n",
    "            'repeated_words': re.compile(r'\\b(\\w+)(?:\\s+\\1\\b)+'),\n",
    "            'whitespace': re.compile(r'\\s+'),\n",
    "            'punct_space': re.compile(r'\\s+([.,:])'),\n",
    "            'repeated_punct': re.compile(r'([.,])\\1+'),\n",
    "        }\n",
    "        self.subscript_trans = str.maketrans(\"‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ\", \"0123456789\")\n",
    "        self.special_chars_trans = str.maketrans('·∏´·∏™', 'hH')\n",
    "        self.forbidden_trans = str.maketrans('', '', '!?()\"‚Äî‚Äî<>‚åà‚åã‚åä[]+ æ/;')\n",
    "    \n",
    "    def postprocess_batch(self, translations: List[str]) -> List[str]:\n",
    "        s = pd.Series(translations)\n",
    "        s[~s.apply(lambda x: isinstance(x, str) and bool(x.strip()))] = \"\"\n",
    "        \n",
    "        s = s.str.translate(self.special_chars_trans)\n",
    "        s = s.str.translate(self.subscript_trans)\n",
    "        s = s.str.replace(self.patterns['whitespace'], ' ', regex=True).str.strip()\n",
    "        \n",
    "        if self.aggressive:\n",
    "            s = s.str.replace(self.patterns['gap'], '<gap>', regex=True)\n",
    "            s = s.str.replace(self.patterns['big_gap'], '<big_gap>', regex=True)\n",
    "            s = s.str.replace('<gap> <gap>', '<big_gap>', regex=False)\n",
    "            s = s.str.replace('<big_gap> <big_gap>', '<big_gap>', regex=False)\n",
    "            s = s.str.replace(self.patterns['annotations'], '', regex=True)\n",
    "            \n",
    "            # Protect gaps during forbidden char removal\n",
    "            s = s.str.replace('<gap>', '\\x00GAP\\x00', regex=False)\n",
    "            s = s.str.replace('<big_gap>', '\\x00BIG\\x00', regex=False)\n",
    "            s = s.str.translate(self.forbidden_trans)\n",
    "            s = s.str.replace('\\x00GAP\\x00', ' <gap> ', regex=False)\n",
    "            s = s.str.replace('\\x00BIG\\x00', ' <big_gap> ', regex=False)\n",
    "            \n",
    "            # Fractions\n",
    "            for pat, repl in [(r'(\\d+)\\.5\\b', r'\\1¬Ω'), (r'\\b0\\.5\\b', '¬Ω'),\n",
    "                              (r'(\\d+)\\.25\\b', r'\\1¬º'), (r'\\b0\\.25\\b', '¬º'),\n",
    "                              (r'(\\d+)\\.75\\b', r'\\1¬æ'), (r'\\b0\\.75\\b', '¬æ')]:\n",
    "                s = s.str.replace(pat, repl, regex=True)\n",
    "            \n",
    "            # Remove repeated words/ngrams\n",
    "            s = s.str.replace(self.patterns['repeated_words'], r'\\1', regex=True)\n",
    "            for n in range(4, 1, -1):\n",
    "                pat = r'\\b((?:\\w+\\s+){' + str(n-1) + r'}\\w+)(?:\\s+\\1\\b)+'\n",
    "                s = s.str.replace(pat, r'\\1', regex=True)\n",
    "            \n",
    "            s = s.str.replace(self.patterns['punct_space'], r'\\1', regex=True)\n",
    "            s = s.str.replace(self.patterns['repeated_punct'], r'\\1', regex=True)\n",
    "            s = s.str.replace(self.patterns['whitespace'], ' ', regex=True)\n",
    "            s = s.str.strip().str.strip('-').str.strip()\n",
    "        \n",
    "        return s.tolist()\n",
    "\n",
    "# ============================================================\n",
    "# DATASET & SAMPLER\n",
    "# ============================================================\n",
    "class AkkadianDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, preprocessor):\n",
    "        self.sample_ids = df['id'].tolist()\n",
    "        preprocessed = preprocessor.preprocess_batch(df['transliteration'].tolist())\n",
    "        self.input_texts = [\"translate Akkadian to English: \" + t for t in preprocessed]\n",
    "        logger.info(f\"Dataset: {len(self.sample_ids)} samples\")\n",
    "    \n",
    "    def __len__(self): return len(self.sample_ids)\n",
    "    def __getitem__(self, i): return self.sample_ids[i], self.input_texts[i]\n",
    "\n",
    "class BucketBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size: int, num_buckets: int = 4):\n",
    "        lengths = [len(t.split()) for _, t in dataset]\n",
    "        sorted_idx = sorted(range(len(lengths)), key=lambda i: lengths[i])\n",
    "        bucket_size = max(1, len(sorted_idx) // num_buckets)\n",
    "        self.buckets = [sorted_idx[i*bucket_size:(i+1)*bucket_size if i < num_buckets-1 else None]\n",
    "                        for i in range(num_buckets) if sorted_idx[i*bucket_size:]]\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for bucket in self.buckets:\n",
    "            for i in range(0, len(bucket), self.batch_size):\n",
    "                yield bucket[i:i+self.batch_size]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum((len(b) + self.batch_size - 1) // self.batch_size for b in self.buckets)\n",
    "\n",
    "# ============================================================\n",
    "# INFERENCE ENGINE\n",
    "# ============================================================\n",
    "class UltraInferenceEngine:\n",
    "    def __init__(self, config: UltraConfig):\n",
    "        self.config = config\n",
    "        self.preprocessor = OptimizedPreprocessor()\n",
    "        self.postprocessor = VectorizedPostprocessor(aggressive=config.aggressive_postprocessing)\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        logger.info(f\"Loading model: {self.config.model_path}\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(self.config.device).eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)\n",
    "        logger.info(f\"Model: {sum(p.numel() for p in self.model.parameters()):,} params\")\n",
    "        \n",
    "        if self.config.use_better_transformer and torch.cuda.is_available():\n",
    "            try:\n",
    "                from optimum.bettertransformer import BetterTransformer\n",
    "                self.model = BetterTransformer.transform(self.model)\n",
    "                logger.info(\"‚úÖ BetterTransformer applied\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è BetterTransformer skipped: {e}\")\n",
    "                self.config.use_better_transformer = False\n",
    "    \n",
    "    def _collate_fn(self, batch):\n",
    "        ids = [s[0] for s in batch]\n",
    "        texts = [s[1] for s in batch]\n",
    "        tok = self.tokenizer(texts, max_length=self.config.max_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return ids, tok\n",
    "    \n",
    "    def run_inference(self, test_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        logger.info(\"üöÄ Starting inference\")\n",
    "        \n",
    "        # Simplify for tiny datasets\n",
    "        if len(test_df) < self.config.tiny_dataset_threshold:\n",
    "            self.config.use_bucket_batching = False\n",
    "            self.config.num_workers = 0\n",
    "        \n",
    "        dataset = AkkadianDataset(test_df, self.preprocessor)\n",
    "        \n",
    "        if self.config.use_bucket_batching:\n",
    "            sampler = BucketBatchSampler(dataset, self.config.batch_size, self.config.num_buckets)\n",
    "            loader = DataLoader(dataset, batch_sampler=sampler, num_workers=self.config.num_workers,\n",
    "                                collate_fn=self._collate_fn, pin_memory=True)\n",
    "        else:\n",
    "            loader = DataLoader(dataset, batch_size=self.config.batch_size, shuffle=False,\n",
    "                                num_workers=self.config.num_workers, collate_fn=self._collate_fn, pin_memory=True)\n",
    "        \n",
    "        gen_cfg = {\n",
    "            \"max_new_tokens\": self.config.max_new_tokens,\n",
    "            \"length_penalty\": self.config.length_penalty,\n",
    "            \"early_stopping\": self.config.early_stopping,\n",
    "            \"use_cache\": True,\n",
    "            \"num_beams\": self.config.num_beams,\n",
    "        }\n",
    "        if self.config.no_repeat_ngram_size > 0:\n",
    "            gen_cfg[\"no_repeat_ngram_size\"] = self.config.no_repeat_ngram_size\n",
    "        \n",
    "        results = []\n",
    "        with torch.inference_mode():\n",
    "            for batch_ids, tok in tqdm(loader, desc=\"üöÄ Translating\"):\n",
    "                input_ids = tok.input_ids.to(self.config.device)\n",
    "                attn_mask = tok.attention_mask.to(self.config.device)\n",
    "                \n",
    "                # Adaptive beams: fewer for short sequences\n",
    "                if self.config.use_adaptive_beams:\n",
    "                    avg_len = attn_mask.sum(dim=1).float().mean().item()\n",
    "                    gen_cfg[\"num_beams\"] = max(4, self.config.num_beams // 2) if avg_len < 100 else self.config.num_beams\n",
    "                \n",
    "                if self.config.use_mixed_precision:\n",
    "                    with autocast():\n",
    "                        outputs = self.model.generate(input_ids=input_ids, attention_mask=attn_mask, **gen_cfg)\n",
    "                else:\n",
    "                    outputs = self.model.generate(input_ids=input_ids, attention_mask=attn_mask, **gen_cfg)\n",
    "                \n",
    "                translations = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                cleaned = self.postprocessor.postprocess_batch(translations)\n",
    "                results.extend(zip(batch_ids, cleaned))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        logger.info(\"‚úÖ Inference complete\")\n",
    "        return pd.DataFrame(results, columns=['id', 'translation'])\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "logger.info(f\"Loading test data: {config.test_data_path}\")\n",
    "test_df = pd.read_csv(config.test_data_path, encoding='utf-8')\n",
    "logger.info(f\"‚úÖ Loaded {len(test_df)} samples\")\n",
    "\n",
    "engine = UltraInferenceEngine(config)\n",
    "results_df = engine.run_inference(test_df)\n",
    "\n",
    "# Save submission.csv\n",
    "output_path = Path(config.output_dir) / 'submission.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "logger.info(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "# Validation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SUBMISSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows: {len(results_df)}\")\n",
    "print(f\"Empty translations: {results_df['translation'].str.strip().eq('').sum()}\")\n",
    "print(f\"Length range: [{results_df['translation'].str.len().min()}, {results_df['translation'].str.len().max()}]\")\n",
    "print(f\"\\nFirst 3:\")\n",
    "print(results_df.head(3).to_string(index=False))\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9366631,
     "sourceId": 14661932,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 60.159626,
   "end_time": "2026-02-01T10:59:36.958951",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-01T10:58:36.799325",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1392690aeb264862a16639cbebb408d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "57eb66e975d143c1a4b793685f82b83a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "774e31af40a8496097512b32fa2a2988": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b6dec44132b49b7b8b62c930908a5b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "afa68b129f66474799c5d11fe31d0039": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f9e71453fe2546cd90a02ded702ed64d",
        "IPY_MODEL_d7baccf5c1b140729f5f1654d961b550",
        "IPY_MODEL_f09e6d1fe00549a4859c1e6b4bc7a071"
       ],
       "layout": "IPY_MODEL_d90d0ad182814679a00205c77ee13b13",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c46a530efaea4875ae7e07d1ed5baba7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7baccf5c1b140729f5f1654d961b550": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c46a530efaea4875ae7e07d1ed5baba7",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_df85e73cfb8840e6b26806c438d74425",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "d90d0ad182814679a00205c77ee13b13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df85e73cfb8840e6b26806c438d74425": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f09e6d1fe00549a4859c1e6b4bc7a071": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7b6dec44132b49b7b8b62c930908a5b0",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_1392690aeb264862a16639cbebb408d5",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá1/1‚Äá[00:05&lt;00:00,‚Äá‚Äá5.06s/it]"
      }
     },
     "f9e71453fe2546cd90a02ded702ed64d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_774e31af40a8496097512b32fa2a2988",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_57eb66e975d143c1a4b793685f82b83a",
       "tabbable": null,
       "tooltip": null,
       "value": "üöÄ‚ÄáTranslating:‚Äá100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
