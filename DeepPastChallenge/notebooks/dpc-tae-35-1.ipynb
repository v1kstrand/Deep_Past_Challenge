{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a10d600",
   "metadata": {
    "_cell_guid": "75670507-337c-43ba-a050-c3825554a650",
    "_uuid": "66affc1e-31bb-41d1-998d-6db28b668276",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-03T12:16:17.308594Z",
     "iopub.status.busy": "2026-02-03T12:16:17.308299Z",
     "iopub.status.idle": "2026-02-03T12:17:15.088469Z",
     "shell.execute_reply": "2026-02-03T12:17:15.087443Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 57.787476,
     "end_time": "2026-02-03T12:17:15.089924",
     "exception": false,
     "start_time": "2026-02-03T12:16:17.302448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 12:16:31,724 - INFO - Logging initialized\n",
      "2026-02-03 12:16:31,748 - INFO - Configuration:\n",
      "2026-02-03 12:16:31,748 - INFO -   Device: cuda\n",
      "2026-02-03 12:16:31,749 - INFO -   Batch size: 8\n",
      "2026-02-03 12:16:31,749 - INFO -   Beams: 8\n",
      "2026-02-03 12:16:31,750 - INFO - Optimizations:\n",
      "2026-02-03 12:16:31,750 - INFO -   Mixed Precision: True\n",
      "2026-02-03 12:16:31,751 - INFO -   BetterTransformer: True\n",
      "2026-02-03 12:16:31,752 - INFO -   Bucket Batching: True\n",
      "2026-02-03 12:16:31,753 - INFO -   Vectorized Postproc: True\n",
      "2026-02-03 12:16:31,754 - INFO -   Adaptive Beams: True\n",
      "2026-02-03 12:16:31,754 - INFO - Loading test data from /kaggle/input/deep-past-initiative-machine-translation/test.csv\n",
      "2026-02-03 12:16:31,768 - INFO - ‚úÖ Loaded 4 test samples\n",
      "2026-02-03 12:16:31,777 - INFO - üöÄ Starting ENSEMBLE inference\n",
      "2026-02-03 12:16:31,778 - INFO - Dataset created with 4 samples\n",
      "2026-02-03 12:16:31,779 - INFO - \n",
      "=== Model 1/1: byt5-akkadian-optimized-34x ===\n",
      "2026-02-03 12:16:31,780 - INFO - Loading model from /kaggle/input/final-byt5/byt5-akkadian-optimized-34x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "GPU Memory: 17.06 GB\n",
      "‚ùå BetterTransformer NOT available\n",
      "\n",
      "First 5 samples:\n",
      "   id   text_id  line_start  line_end  \\\n",
      "0   0  332fda50           1         7   \n",
      "1   1  332fda50           7        14   \n",
      "2   2  332fda50          14        24   \n",
      "3   3  332fda50          25        30   \n",
      "\n",
      "                                     transliteration  \n",
      "0  um-ma k√†-ru-um k√†-ni-ia-ma a-na aa-q√≠-il‚Ä¶ da-t...  \n",
      "1  i-na mup-p√¨-im aa a-lim(ki) ia-t√π u‚Äû-m√¨-im a-n...  \n",
      "2  ki-ma mup-p√¨-ni ta-√°a-me-a-ni a-ma-kam lu a-na...  \n",
      "3  me-+e-er mup-p√¨-ni a-na k√†-ar k√†-ar-ma √∫ wa-ba...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 12:16:34.237577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770120994.459841      25 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770120994.515559      25 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770120995.005831      25 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770120995.005878      25 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770120995.005881      25 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770120995.005883      25 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-03 12:17:03,777 - INFO - Model loaded: 581,653,248 parameters\n",
      "2026-02-03 12:17:03,779 - WARNING - ‚ö†Ô∏è  'optimum' not installed, skipping BetterTransformer\n",
      "2026-02-03 12:17:03,779 - WARNING -    Install with: !pip install optimum\n",
      "2026-02-03 12:17:03,780 - INFO - Created 4 buckets:\n",
      "2026-02-03 12:17:03,780 - INFO -   Bucket 0: 1 samples, length range [20, 20]\n",
      "2026-02-03 12:17:03,781 - INFO -   Bucket 1: 1 samples, length range [20, 20]\n",
      "2026-02-03 12:17:03,781 - INFO -   Bucket 2: 1 samples, length range [23, 23]\n",
      "2026-02-03 12:17:03,782 - INFO -   Bucket 3: 1 samples, length range [38, 38]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260df5fb382f4860951c244d9908b4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üöÄ Translating (byt5-akkadian-optimized-34x):   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 12:17:15,054 - INFO - ‚ôªÔ∏è  Model unloaded to release memory\n",
      "2026-02-03 12:17:15,055 - INFO - \n",
      "üó≥Ô∏è  Performing Weighted Strength Vote Ensemble...\n",
      "2026-02-03 12:17:15,056 - INFO - ‚úÖ Ensemble Inference completed\n",
      "2026-02-03 12:17:15,069 - INFO - ‚úÖ Submission saved to /kaggle/working/submission.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "üìä VALIDATION REPORT\n",
      "============================================================\n",
      "\\nEmpty: 0 (0.00%)\n",
      "\\nüìè Length stats:\n",
      "   Mean: 160.0, Median: 164.0\n",
      "   Min: 70, Max: 242\n",
      "\\nüìù Sample translations:\n",
      "   ID    0: Thus says the Kanesh colony: Speak to our messengers every single day.\n",
      "   ID    2: As soon as you hear our letter, there, either he gave anything to the ...\n",
      "   ID    3: I sent our tablet to every single colony and the storeroom. He has giv...\n",
      "\\n============================================================\\n\n",
      "\n",
      "============================================================\n",
      "üéâ ULTRA-OPTIMIZED INFERENCE COMPLETE!\n",
      "============================================================\n",
      "Submission file: /kaggle/working/submission.csv\n",
      "Config file: /kaggle/working/ultra_config.json\n",
      "Log file: /kaggle/working/inference_ultra.log\n",
      "Total translations: 4\n",
      "============================================================\n",
      "Submission shape: (4, 2)\n",
      "\n",
      "First 10 translations:\n",
      "   id                                        translation\n",
      "0   0  Thus says the Kanesh colony: Speak to our mess...\n",
      "1   1  As for the tablet of the City, you wrote to me...\n",
      "2   2  As soon as you hear our letter, there, either ...\n",
      "3   3  I sent our tablet to every single colony and t...\n",
      "\n",
      "Last 10 translations:\n",
      "   id                                        translation\n",
      "0   0  Thus says the Kanesh colony: Speak to our mess...\n",
      "1   1  As for the tablet of the City, you wrote to me...\n",
      "2   2  As soon as you hear our letter, there, either ...\n",
      "3   3  I sent our tablet to every single colony and t...\n",
      "\n",
      "Length distribution:\n",
      "count      4.000000\n",
      "mean     160.000000\n",
      "std       71.888803\n",
      "min       70.000000\n",
      "25%      127.000000\n",
      "50%      164.000000\n",
      "75%      197.000000\n",
      "max      242.000000\n",
      "Name: translation, dtype: float64\n",
      "\n",
      "Empty translations: 0\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Ultra-Optimized ByT5 Inference Script (Kaggle-friendly)\n",
    "# ------------------------------------------------------------\n",
    "# Notes:\n",
    "# - Environment variables are set BEFORE importing torch/transformers.\n",
    "# - Designed as a standalone script (no notebook cells).\n",
    "# - Includes optional BetterTransformer (optimum) acceleration.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ---------------------------\n",
    "# 1) System-Level Optimization\n",
    "# ---------------------------\n",
    "\n",
    "# Import standard libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing PyTorch\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ[\"TORCH_CUDNN_V8_API_ENABLED\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Imports & Setup\n",
    "# ---------------------------\n",
    "\n",
    "# Import standard libraries\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Import third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Import Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Import progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Configuration\n",
    "# ---------------------------\n",
    "\n",
    "@dataclass\n",
    "class UltraConfig:\n",
    "    # ============ PATHS ============\n",
    "    test_data_path: str = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
    "    model_paths: List[str] = field(default_factory=lambda: [\"/kaggle/input/final-byt5/byt5-akkadian-optimized-34x\"])\n",
    "    output_dir: str = \"/kaggle/working/\"\n",
    "\n",
    "    # ============ PROCESSING ============\n",
    "    max_length: int = 512\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # ============ GENERATION ============\n",
    "    num_beams: int = 8\n",
    "    max_new_tokens: int = 512\n",
    "    length_penalty: float = 1.3\n",
    "    early_stopping: bool = True\n",
    "    no_repeat_ngram_size: int = 0\n",
    "\n",
    "    # ============ OPTIMIZATIONS ============\n",
    "    use_mixed_precision: bool = True\n",
    "    use_better_transformer: bool = True\n",
    "    use_bucket_batching: bool = True\n",
    "    use_vectorized_postproc: bool = True\n",
    "    use_adaptive_beams: bool = True\n",
    "    use_auto_batch_size: bool = False\n",
    "\n",
    "    # ============ OTHER ============\n",
    "    aggressive_postprocessing: bool = True\n",
    "    checkpoint_freq: int = 100\n",
    "    num_buckets: int = 4\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Set device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Create output directory\n",
    "        Path(self.output_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Disable CUDA-dependent optimizations if no GPU\n",
    "        if not torch.cuda.is_available():\n",
    "            self.use_mixed_precision = False\n",
    "            self.use_better_transformer = False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Logging Setup\n",
    "# ---------------------------\n",
    "\n",
    "def setup_logging(output_dir: str) -> logging.Logger:\n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Define log file path\n",
    "    log_file = Path(output_dir) / \"inference_ultra.log\"\n",
    "\n",
    "    # Remove existing handlers\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler(log_file),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Return logger\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Optimized Text Preprocessor\n",
    "# ---------------------------\n",
    "\n",
    "class OptimizedPreprocessor:\n",
    "    # Initialize precompiled regex patterns\n",
    "    def __init__(self):\n",
    "        # Pre-compile patterns\n",
    "        self.patterns = {\n",
    "            \"big_gap\": re.compile(r\"(\\.{3,}|‚Ä¶+|‚Ä¶‚Ä¶)\"),\n",
    "            \"small_gap\": re.compile(r\"(xx+|\\s+x\\s+)\"),\n",
    "        }\n",
    "\n",
    "    # Preprocess a single input text\n",
    "    def preprocess_input_text(self, text: str) -> str:\n",
    "        # Handle NaN\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to string\n",
    "        cleaned_text = str(text)\n",
    "\n",
    "        # Replace gaps\n",
    "        cleaned_text = self.patterns[\"big_gap\"].sub(\"<big_gap>\", cleaned_text)\n",
    "        cleaned_text = self.patterns[\"small_gap\"].sub(\"<gap>\", cleaned_text)\n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "    # Preprocess a batch of texts using vectorized pandas ops\n",
    "    def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "        # Convert to Series and sanitize\n",
    "        s = pd.Series(texts).fillna(\"\")\n",
    "        s = s.astype(str)\n",
    "\n",
    "        # Apply vectorized replacements\n",
    "        s = s.str.replace(self.patterns[\"big_gap\"], \"<big_gap>\", regex=True)\n",
    "        s = s.str.replace(self.patterns[\"small_gap\"], \"<gap>\", regex=True)\n",
    "\n",
    "        return s.tolist()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Vectorized Postprocessor\n",
    "# ---------------------------\n",
    "\n",
    "class VectorizedPostprocessor:\n",
    "    # Initialize postprocessor patterns and translation tables\n",
    "    def __init__(self, aggressive: bool = True):\n",
    "        # Store mode\n",
    "        self.aggressive = aggressive\n",
    "\n",
    "        # Pre-compile patterns\n",
    "        self.patterns = {\n",
    "            \"gap\": re.compile(r\"(\\[x\\]|\\(x\\)|\\bx\\b)\", re.I),\n",
    "            \"big_gap\": re.compile(r\"(\\.{3,}|‚Ä¶|\\[\\.+\\])\"),\n",
    "            \"annotations\": re.compile(r\"\\((fem|plur|pl|sing|singular|plural|\\?|!)\\..\\s*\\w*\\)\", re.I),\n",
    "            \"repeated_words\": re.compile(r\"\\b(\\w+)(?:\\s+\\1\\b)+\"),\n",
    "            \"whitespace\": re.compile(r\"\\s+\"),\n",
    "            \"punct_space\": re.compile(r\"\\s+([.,:])\"),\n",
    "            \"repeated_punct\": re.compile(r\"([.,])\\1+\"),\n",
    "        }\n",
    "\n",
    "        # Create character translation tables\n",
    "        self.subscript_trans = str.maketrans(\"‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ\", \"0123456789\")\n",
    "        self.special_chars_trans = str.maketrans(\"·∏´·∏™\", \"hH\")\n",
    "\n",
    "        # Define forbidden characters\n",
    "        self.forbidden_chars = '!?()\"‚Äî‚Äî<>‚åà‚åã‚åä[]+ æ/;'\n",
    "\n",
    "        # Create forbidden translate table\n",
    "        self.forbidden_trans = str.maketrans(\"\", \"\", self.forbidden_chars)\n",
    "\n",
    "    # Vectorized postprocessing\n",
    "    def postprocess_batch(self, translations: List[str]) -> List[str]:\n",
    "        # Convert to Series\n",
    "        s = pd.Series(translations)\n",
    "\n",
    "        # Validate entries\n",
    "        valid_mask = s.apply(lambda x: isinstance(x, str) and x.strip())\n",
    "        if not valid_mask.all():\n",
    "            s[~valid_mask] = \"\"\n",
    "\n",
    "        # Basic cleaning\n",
    "        s = s.str.translate(self.special_chars_trans)\n",
    "        s = s.str.translate(self.subscript_trans)\n",
    "        s = s.str.replace(self.patterns[\"whitespace\"], \" \", regex=True)\n",
    "        s = s.str.strip()\n",
    "\n",
    "        # Aggressive postprocessing\n",
    "        if self.aggressive:\n",
    "            # Normalize gaps\n",
    "            s = s.str.replace(self.patterns[\"gap\"], \"<gap>\", regex=True)\n",
    "            s = s.str.replace(self.patterns[\"big_gap\"], \"<big_gap>\", regex=True)\n",
    "\n",
    "            # Merge adjacent gaps\n",
    "            s = s.str.replace(\"<gap> <gap>\", \"<big_gap>\", regex=False)\n",
    "            s = s.str.replace(\"<big_gap> <big_gap>\", \"<big_gap>\", regex=False)\n",
    "\n",
    "            # Remove annotations\n",
    "            s = s.str.replace(self.patterns[\"annotations\"], \"\", regex=True)\n",
    "\n",
    "            # Protect gaps during forbidden-character removal\n",
    "            s = s.str.replace(\"<gap>\", \"\\x00GAP\\x00\", regex=False)\n",
    "            s = s.str.replace(\"<big_gap>\", \"\\x00BIG\\x00\", regex=False)\n",
    "\n",
    "            # Remove forbidden characters\n",
    "            s = s.str.translate(self.forbidden_trans)\n",
    "\n",
    "            # Restore gaps\n",
    "            s = s.str.replace(\"\\x00GAP\\x00\", \" <gap> \", regex=False)\n",
    "            s = s.str.replace(\"\\x00BIG\\x00\", \" <big_gap> \", regex=False)\n",
    "\n",
    "            # Fractions\n",
    "            s = s.str.replace(r\"(\\d+)\\.5\\b\", r\"\\1¬Ω\", regex=True)\n",
    "            s = s.str.replace(r\"\\b0\\.5\\b\", \"¬Ω\", regex=True)\n",
    "            s = s.str.replace(r\"(\\d+)\\.25\\b\", r\"\\1¬º\", regex=True)\n",
    "            s = s.str.replace(r\"\\b0\\.25\\b\", \"¬º\", regex=True)\n",
    "            s = s.str.replace(r\"(\\d+)\\.75\\b\", r\"\\1¬æ\", regex=True)\n",
    "            s = s.str.replace(r\"\\b0\\.75\\b\", \"¬æ\", regex=True)\n",
    "\n",
    "            # Remove repeated words\n",
    "            s = s.str.replace(self.patterns[\"repeated_words\"], r\"\\1\", regex=True)\n",
    "\n",
    "            # Remove repeated n-grams (4 -> 2)\n",
    "            for n in range(4, 1, -1):\n",
    "                pattern = r\"\\b((?:\\w+\\s+){\" + str(n - 1) + r\"}\\w+)(?:\\s+\\1\\b)+\"\n",
    "                s = s.str.replace(pattern, r\"\\1\", regex=True)\n",
    "\n",
    "            # Fix punctuation spacing and repeats\n",
    "            s = s.str.replace(self.patterns[\"punct_space\"], r\"\\1\", regex=True)\n",
    "            s = s.str.replace(self.patterns[\"repeated_punct\"], r\"\\1\", regex=True)\n",
    "\n",
    "            # Final whitespace cleanup\n",
    "            s = s.str.replace(self.patterns[\"whitespace\"], \" \", regex=True)\n",
    "            s = s.str.strip().str.strip(\"-\").str.strip()\n",
    "\n",
    "        return s.tolist()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Bucket Batch Sampler\n",
    "# ---------------------------\n",
    "\n",
    "class BucketBatchSampler(Sampler):\n",
    "    # Initialize sampler\n",
    "    def __init__(self, dataset: Dataset, batch_size: int, num_buckets: int, logger: logging.Logger, shuffle: bool = False):\n",
    "        # Store settings\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.logger = logger\n",
    "\n",
    "        # Compute lengths for bucketing\n",
    "        lengths = [len(text.split()) for _, text in dataset]\n",
    "\n",
    "        # Sort indices by length\n",
    "        sorted_indices = sorted(range(len(lengths)), key=lambda i: lengths[i])\n",
    "\n",
    "        # Create buckets\n",
    "        bucket_size = max(1, len(sorted_indices) // max(1, num_buckets))\n",
    "        self.buckets = []\n",
    "\n",
    "        for i in range(num_buckets):\n",
    "            start = i * bucket_size\n",
    "            end = None if i == num_buckets - 1 else (i + 1) * bucket_size\n",
    "            self.buckets.append(sorted_indices[start:end])\n",
    "\n",
    "        # Log bucket details\n",
    "        self.logger.info(f\"Created {num_buckets} buckets:\")\n",
    "        for i, bucket in enumerate(self.buckets):\n",
    "            bucket_lengths = [lengths[idx] for idx in bucket] if len(bucket) > 0 else [0]\n",
    "            self.logger.info(\n",
    "                f\"  Bucket {i}: {len(bucket)} samples, length range [{min(bucket_lengths)}, {max(bucket_lengths)}]\"\n",
    "            )\n",
    "\n",
    "    # Yield batches\n",
    "    def __iter__(self):\n",
    "        for bucket in self.buckets:\n",
    "            if self.shuffle:\n",
    "                random.shuffle(bucket)\n",
    "\n",
    "            for i in range(0, len(bucket), self.batch_size):\n",
    "                yield bucket[i : i + self.batch_size]\n",
    "\n",
    "    # Return number of batches\n",
    "    def __len__(self):\n",
    "        total = 0\n",
    "        for bucket in self.buckets:\n",
    "            total += (len(bucket) + self.batch_size - 1) // self.batch_size\n",
    "        return total\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Dataset\n",
    "# ---------------------------\n",
    "\n",
    "class AkkadianDataset(Dataset):\n",
    "    # Initialize dataset\n",
    "    def __init__(self, dataframe: pd.DataFrame, preprocessor: OptimizedPreprocessor, logger: logging.Logger):\n",
    "        # Store ids\n",
    "        self.sample_ids = dataframe[\"id\"].tolist()\n",
    "\n",
    "        # Preprocess in batch\n",
    "        raw_texts = dataframe[\"transliteration\"].tolist()\n",
    "        preprocessed = preprocessor.preprocess_batch(raw_texts)\n",
    "\n",
    "        # Add task prefix\n",
    "        self.input_texts = [\"translate Akkadian to English: \" + text for text in preprocessed]\n",
    "\n",
    "        # Log dataset info\n",
    "        logger.info(f\"Dataset created with {len(self.sample_ids)} samples\")\n",
    "\n",
    "    # Return size\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    # Return item\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.sample_ids[index], self.input_texts[index]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Ultra-Optimized Inference Engine\n",
    "# ---------------------------\n",
    "\n",
    "class UltraInferenceEngine:\n",
    "    # Initialize engine\n",
    "    def __init__(self, config: UltraConfig, logger: logging.Logger):\n",
    "        # Store config and logger\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "\n",
    "        # Create helpers\n",
    "        self.preprocessor = OptimizedPreprocessor()\n",
    "        self.postprocessor = VectorizedPostprocessor(aggressive=config.aggressive_postprocessing)\n",
    "\n",
    "        # Initialize results\n",
    "        self.results = []\n",
    "\n",
    "    # Load and optimize single model\n",
    "    def _load_single_model(self, model_path: str):\n",
    "        # Log model load\n",
    "        self.logger.info(f\"Loading model from {model_path}\")\n",
    "\n",
    "        # Load model\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        self.model = self.model.to(self.config.device)\n",
    "        self.model = self.model.eval()\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        # Log parameter count\n",
    "        num_params = sum(p.numel() for p in self.model.parameters())\n",
    "        self.logger.info(f\"Model loaded: {num_params:,} parameters\")\n",
    "\n",
    "        # Apply BetterTransformer if enabled\n",
    "        if self.config.use_better_transformer and torch.cuda.is_available():\n",
    "            try:\n",
    "                # Import optimum lazily\n",
    "                from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "                # Apply transformation\n",
    "                self.logger.info(\"Applying BetterTransformer...\")\n",
    "                self.model = BetterTransformer.transform(self.model)\n",
    "                self.logger.info(\"‚úÖ BetterTransformer applied (20-50% speedup)\")\n",
    "            except ImportError:\n",
    "                self.logger.warning(\"‚ö†Ô∏è  'optimum' not installed, skipping BetterTransformer\")\n",
    "                self.logger.warning(\"   Install with: !pip install optimum\")\n",
    "            except Exception as exc:\n",
    "                self.logger.warning(f\"‚ö†Ô∏è  BetterTransformer failed: {exc}\")\n",
    "\n",
    "    def _unload_model(self):\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        self.logger.info(\"‚ôªÔ∏è  Model unloaded to release memory\")\n",
    "\n",
    "    # Collate function for DataLoader\n",
    "    def _collate_fn(self, batch_samples):\n",
    "        # Extract ids and texts\n",
    "        batch_ids = [s[0] for s in batch_samples]\n",
    "        batch_texts = [s[1] for s in batch_samples]\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized = self.tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=self.config.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return batch_ids, tokenized\n",
    "\n",
    "    # Adaptive beam selection\n",
    "    def _get_adaptive_beam_size(self, attention_mask: torch.Tensor) -> int:\n",
    "        # Return fixed beams if disabled\n",
    "        if not self.config.use_adaptive_beams:\n",
    "            return self.config.num_beams\n",
    "\n",
    "        # Compute lengths\n",
    "        lengths = attention_mask.sum(dim=1)\n",
    "\n",
    "        # Choose beams based on length\n",
    "        short_beams = max(4, self.config.num_beams // 2)\n",
    "        beam_sizes = torch.where(\n",
    "            lengths < 100,\n",
    "            torch.tensor(short_beams, device=lengths.device),\n",
    "            torch.tensor(self.config.num_beams, device=lengths.device),\n",
    "        )\n",
    "\n",
    "        # Use first element (batch-wise adaptive is possible, but keep simple/fast)\n",
    "        return int(beam_sizes[0].item())\n",
    "\n",
    "    # Save periodic checkpoints\n",
    "    def _save_checkpoint(self, current_results):\n",
    "        # Checkpoint only when frequency matches\n",
    "        if len(current_results) > 0 and len(current_results) % self.config.checkpoint_freq == 0:\n",
    "            checkpoint_path = Path(self.config.output_dir) / f\"checkpoint_{len(current_results)}.csv\"\n",
    "\n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(current_results, columns=[\"id\", \"translation\"])\n",
    "\n",
    "            # Save\n",
    "            df.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "            # Log\n",
    "            self.logger.info(f\"üíæ Checkpoint: {len(current_results)} translations\")\n",
    "\n",
    "    # Optional: auto-tune batch size\n",
    "    def find_optimal_batch_size(self, dataset: Dataset, start_bs: int = 32) -> int:\n",
    "        # Log\n",
    "        self.logger.info(\"üîç Finding optimal batch size...\")\n",
    "\n",
    "        # Initialize binary search\n",
    "        max_bs = start_bs\n",
    "        min_bs = 1\n",
    "\n",
    "        # Binary search loop\n",
    "        while max_bs - min_bs > 1:\n",
    "            # Choose midpoint\n",
    "            test_bs = (max_bs + min_bs) // 2\n",
    "\n",
    "            try:\n",
    "                # Build test batch\n",
    "                test_batch = [dataset[i] for i in range(min(test_bs, len(dataset)))]\n",
    "\n",
    "                # Collate\n",
    "                _, inputs = self._collate_fn(test_batch)\n",
    "\n",
    "                # Run a tiny generation\n",
    "                with torch.inference_mode():\n",
    "                    if self.config.use_mixed_precision:\n",
    "                        with autocast():\n",
    "                            _ = self.model.generate(\n",
    "                                input_ids=inputs.input_ids.to(self.config.device),\n",
    "                                attention_mask=inputs.attention_mask.to(self.config.device),\n",
    "                                num_beams=self.config.num_beams,\n",
    "                                max_new_tokens=64,\n",
    "                                use_cache=True,\n",
    "                            )\n",
    "                    else:\n",
    "                        _ = self.model.generate(\n",
    "                            input_ids=inputs.input_ids.to(self.config.device),\n",
    "                            attention_mask=inputs.attention_mask.to(self.config.device),\n",
    "                            num_beams=self.config.num_beams,\n",
    "                            max_new_tokens=64,\n",
    "                            use_cache=True,\n",
    "                        )\n",
    "\n",
    "                # Update min bound\n",
    "                min_bs = test_bs\n",
    "                self.logger.info(f\"  ‚úÖ Batch size {test_bs} works\")\n",
    "\n",
    "                # Cleanup\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except RuntimeError as exc:\n",
    "                # Handle OOM\n",
    "                if \"out of memory\" in str(exc).lower():\n",
    "                    max_bs = test_bs\n",
    "                    self.logger.info(f\"  ‚ùå Batch size {test_bs} OOM\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        # Log result\n",
    "        self.logger.info(f\"üéØ Optimal batch size: {min_bs}\")\n",
    "        return min_bs\n",
    "\n",
    "    def _run_single_model_pass(self, dataset: Dataset, model_name: str) -> List[Tuple[str, str, float]]:\n",
    "        # Auto-tune batch size if enabled (only for first model or always? let's do always to be safe)\n",
    "        if self.config.use_auto_batch_size:\n",
    "            optimal_bs = self.find_optimal_batch_size(dataset)\n",
    "            self.config.batch_size = optimal_bs\n",
    "        \n",
    "        # Create DataLoader\n",
    "        batch_sampler = None\n",
    "        if self.config.use_bucket_batching:\n",
    "             batch_sampler = BucketBatchSampler(\n",
    "                dataset=dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                num_buckets=self.config.num_buckets,\n",
    "                logger=self.logger,\n",
    "                shuffle=False,\n",
    "            )\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            batch_size=self.config.batch_size if not batch_sampler else 1,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers,\n",
    "            collate_fn=self._collate_fn,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=2,\n",
    "            persistent_workers=True if self.config.num_workers > 0 else False,\n",
    "        )\n",
    "\n",
    "        # Build base generation config\n",
    "        base_gen_config = {\n",
    "            \"max_new_tokens\": self.config.max_new_tokens,\n",
    "            \"length_penalty\": self.config.length_penalty,\n",
    "            \"early_stopping\": self.config.early_stopping,\n",
    "            \"use_cache\": True,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "        }\n",
    "\n",
    "        # Add no-repeat constraint if requested\n",
    "        if self.config.no_repeat_ngram_size > 0:\n",
    "            base_gen_config[\"no_repeat_ngram_size\"] = self.config.no_repeat_ngram_size\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Inference loop\n",
    "        with torch.inference_mode():\n",
    "            for batch_idx, (batch_ids, tokenized) in enumerate(tqdm(dataloader, desc=f\"üöÄ Translating ({model_name})\")):\n",
    "                try:\n",
    "                    # Move inputs\n",
    "                    input_ids = tokenized.input_ids.to(self.config.device)\n",
    "                    attention_mask = tokenized.attention_mask.to(self.config.device)\n",
    "\n",
    "                    # Adaptive beams\n",
    "                    beam_size = self._get_adaptive_beam_size(attention_mask)\n",
    "                    gen_config = dict(base_gen_config)\n",
    "                    gen_config[\"num_beams\"] = beam_size\n",
    "\n",
    "                    # Generate\n",
    "                    if self.config.use_mixed_precision:\n",
    "                        with autocast():\n",
    "                            outputs = self.model.generate(\n",
    "                                input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                **gen_config,\n",
    "                            )\n",
    "                    else:\n",
    "                        outputs = self.model.generate(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            **gen_config,\n",
    "                        )\n",
    "\n",
    "                    # Extract sequences\n",
    "                    if hasattr(outputs, 'sequences'):\n",
    "                        sequences = outputs.sequences\n",
    "                    else:\n",
    "                        sequences = outputs # Fallback if list or tensor directly\n",
    "\n",
    "                    # Extract scores (log_probs)\n",
    "                    if hasattr(outputs, 'sequences_scores'):\n",
    "                        scores = outputs.sequences_scores.cpu().numpy().tolist()\n",
    "                    else:\n",
    "                        # Fallback: assign 0.0 (neutral score)\n",
    "                        scores = [0.0] * len(sequences)\n",
    "\n",
    "                    # Decode\n",
    "                    translations = self.tokenizer.batch_decode(sequences, skip_special_tokens=True)\n",
    "\n",
    "                    # Postprocess\n",
    "                    if self.config.use_vectorized_postproc:\n",
    "                        cleaned = self.postprocessor.postprocess_batch(translations)\n",
    "                    else:\n",
    "                        cleaned = [self.postprocessor.postprocess_batch([t])[0] for t in translations]\n",
    "\n",
    "                    # Store (id, text, score)\n",
    "                    results.extend(list(zip(batch_ids, cleaned, scores)))\n",
    "\n",
    "                    # Periodic cache clearing\n",
    "                    if torch.cuda.is_available() and batch_idx % 10 == 0:\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                except Exception as exc:\n",
    "                    self.logger.error(f\"‚ùå Batch {batch_idx} error: {exc}\")\n",
    "                    results.extend([(bid, \"\", -999.0) for bid in batch_ids])\n",
    "        \n",
    "        return results\n",
    "    # Run inference end-to-end\n",
    "    # Run inference end-to-end\n",
    "    def _weighted_vote(self, predictions: List[Tuple[str, float]]) -> str:\n",
    "        if not predictions:\n",
    "            return \"\"\n",
    "            \n",
    "        # Group by translation\n",
    "        score_map = defaultdict(float)\n",
    "        \n",
    "        for text, score in predictions:\n",
    "            # Score is log_prob. Convert to prob/strength.\n",
    "            try:\n",
    "                weight = np.exp(score) \n",
    "            except:\n",
    "                weight = 0.0\n",
    "            \n",
    "            score_map[text] += weight\n",
    "            \n",
    "        # Find max\n",
    "        if not score_map:\n",
    "            return \"\"\n",
    "        best_text = max(score_map.items(), key=lambda x: x[1])[0]\n",
    "        return best_text\n",
    "\n",
    "    def run_inference(self, test_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Log start\n",
    "        self.logger.info(\"üöÄ Starting ENSEMBLE inference\")\n",
    "\n",
    "        # Create dataset\n",
    "        dataset = AkkadianDataset(test_df, self.preprocessor, self.logger)\n",
    "\n",
    "        all_predictions = defaultdict(list)\n",
    "        \n",
    "        # Loop through models\n",
    "        for i, model_path in enumerate(self.config.model_paths):\n",
    "            model_name = Path(model_path).name\n",
    "            self.logger.info(f\"\\n=== Model {i+1}/{len(self.config.model_paths)}: {model_name} ===\")\n",
    "            \n",
    "            # Load\n",
    "            self._load_single_model(model_path)\n",
    "            \n",
    "            # Predict\n",
    "            model_results = self._run_single_model_pass(dataset, model_name=model_name)\n",
    "            \n",
    "            # Aggregate\n",
    "            for unique_id, translation, score in model_results:\n",
    "                all_predictions[unique_id].append((translation, score))\n",
    "                \n",
    "            # Unload\n",
    "            self._unload_model()\n",
    "\n",
    "        # Voting\n",
    "        self.logger.info(\"\\nüó≥Ô∏è  Performing Weighted Strength Vote Ensemble...\")\n",
    "        final_results = []\n",
    "        # Sort by ID to ensure deterministic order logic\n",
    "        try:\n",
    "             sorted_keys = sorted(all_predictions.keys())\n",
    "        except:\n",
    "             sorted_keys = all_predictions.keys()\n",
    "             \n",
    "        for unique_id in sorted_keys:\n",
    "             preds = all_predictions[unique_id] # List of (text, score)\n",
    "             winner = self._weighted_vote(preds)\n",
    "             final_results.append((unique_id, winner))\n",
    "\n",
    "        # Build results DataFrame\n",
    "        results_df = pd.DataFrame(final_results, columns=[\"id\", \"translation\"])\n",
    "        \n",
    "        # Log completion\n",
    "        self.logger.info(\"‚úÖ Ensemble Inference completed\")\n",
    "\n",
    "        # Validate\n",
    "        self._validate_results(results_df)\n",
    "\n",
    "        return results_df\n",
    "    def _majority_vote(self, translations: List[str]) -> str:\n",
    "        if not translations:\n",
    "            return \"\"\n",
    "        # Simple majority. In case of tie, Counter.most_common(1) returns the first encountered.\n",
    "        counts = Counter(translations)\n",
    "        return counts.most_common(1)[0][0]\n",
    "\n",
    "    # Print validation report\n",
    "    def _validate_results(self, df: pd.DataFrame):\n",
    "        # Header\n",
    "        print(\"\\\\n\" + \"=\" * 60)\n",
    "        print(\"üìä VALIDATION REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Empty count\n",
    "        empty = df[\"translation\"].astype(str).str.strip().eq(\"\").sum()\n",
    "        print(f\"\\\\nEmpty: {empty} ({(empty / max(1, len(df))) * 100:.2f}%)\")\n",
    "\n",
    "        # Length stats\n",
    "        lengths = df[\"translation\"].astype(str).str.len()\n",
    "        print(\"\\\\nüìè Length stats:\")\n",
    "        print(f\"   Mean: {lengths.mean():.1f}, Median: {lengths.median():.1f}\")\n",
    "        print(f\"   Min: {lengths.min()}, Max: {lengths.max()}\")\n",
    "\n",
    "        # Very short translations\n",
    "        short = ((lengths < 5) & (lengths > 0)).sum()\n",
    "        if short > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  {short} very short translations\")\n",
    "\n",
    "        # Samples\n",
    "        print(\"\\\\nüìù Sample translations:\")\n",
    "\n",
    "        # Choose indices robustly\n",
    "        sample_indices = [0]\n",
    "        if len(df) > 2:\n",
    "            sample_indices.append(len(df) // 2)\n",
    "        if len(df) > 1:\n",
    "            sample_indices.append(len(df) - 1)\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            row = df.iloc[idx]\n",
    "            text = str(row[\"translation\"])\n",
    "            preview = text[:70] + \"...\" if len(text) > 70 else text\n",
    "            print(f\"   ID {int(row['id']):4d}: {preview}\")\n",
    "\n",
    "        print(\"\\\\n\" + \"=\" * 60 + \"\\\\n\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 10) IO Helpers\n",
    "# ---------------------------\n",
    "\n",
    "def print_environment_info():\n",
    "    # Print PyTorch and CUDA info\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {total_mem_gb:.2f} GB\")\n",
    "\n",
    "    # Verify optimum availability (optional)\n",
    "    try:\n",
    "        from optimum.bettertransformer import BetterTransformer  # noqa: F401\n",
    "\n",
    "        print(\"‚úÖ BetterTransformer available!\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå BetterTransformer NOT available\")\n",
    "\n",
    "\n",
    "def save_outputs(\n",
    "    results_df: pd.DataFrame,\n",
    "    config: UltraConfig,\n",
    "    output_dir: str,\n",
    "    logger: logging.Logger,\n",
    "):\n",
    "    # Define output paths\n",
    "    output_path = Path(output_dir) / \"submission.csv\"\n",
    "    config_path = Path(output_dir) / \"ultra_config.json\"\n",
    "\n",
    "    # Save submission\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    logger.info(f\"‚úÖ Submission saved to {output_path}\")\n",
    "\n",
    "    # Build config dict\n",
    "    config_dict = {\n",
    "        \"batch_size\": config.batch_size,\n",
    "        \"num_beams\": config.num_beams,\n",
    "        \"length_penalty\": config.length_penalty,\n",
    "        \"no_repeat_ngram_size\": config.no_repeat_ngram_size,\n",
    "        \"optimizations\": {\n",
    "            \"mixed_precision\": config.use_mixed_precision,\n",
    "            \"better_transformer\": config.use_better_transformer,\n",
    "            \"bucket_batching\": config.use_bucket_batching,\n",
    "            \"vectorized_postproc\": config.use_vectorized_postproc,\n",
    "            \"adaptive_beams\": config.use_adaptive_beams,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Save config json\n",
    "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ ULTRA-OPTIMIZED INFERENCE COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Submission file: {output_path}\")\n",
    "    print(f\"Config file: {config_path}\")\n",
    "    print(f\"Log file: {Path(output_dir) / 'inference_ultra.log'}\")\n",
    "    print(f\"Total translations: {len(results_df)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def inspect_results(output_dir: str):\n",
    "    # Load submission\n",
    "    submission_path = Path(output_dir) / \"submission.csv\"\n",
    "    submission = pd.read_csv(submission_path)\n",
    "\n",
    "    # Print quick view\n",
    "    print(f\"Submission shape: {submission.shape}\")\n",
    "\n",
    "    print(\"\\nFirst 10 translations:\")\n",
    "    print(submission.head(10))\n",
    "\n",
    "    print(\"\\nLast 10 translations:\")\n",
    "    print(submission.tail(10))\n",
    "\n",
    "    # Length statistics\n",
    "    lengths = submission[\"translation\"].astype(str).str.len()\n",
    "    print(\"\\nLength distribution:\")\n",
    "    print(lengths.describe())\n",
    "\n",
    "    # Empty checks\n",
    "    empty = submission[\"translation\"].astype(str).str.strip().eq(\"\").sum()\n",
    "    print(f\"\\nEmpty translations: {empty}\")\n",
    "\n",
    "    if empty > 0:\n",
    "        print(\"\\nEmpty translation IDs:\")\n",
    "        print(submission[submission[\"translation\"].astype(str).str.strip().eq(\"\")][\"id\"].tolist())\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 11) Main\n",
    "# ---------------------------\n",
    "\n",
    "def main():\n",
    "    # Create config\n",
    "    config = UltraConfig()\n",
    "\n",
    "    # Setup logger\n",
    "    logger = setup_logging(config.output_dir)\n",
    "    logger.info(\"Logging initialized\")\n",
    "\n",
    "    # Print environment info\n",
    "    print_environment_info()\n",
    "\n",
    "    # Log configuration\n",
    "    logger.info(\"Configuration:\")\n",
    "    logger.info(f\"  Device: {config.device}\")\n",
    "    logger.info(f\"  Batch size: {config.batch_size}\")\n",
    "    logger.info(f\"  Beams: {config.num_beams}\")\n",
    "    logger.info(\"Optimizations:\")\n",
    "    logger.info(f\"  Mixed Precision: {config.use_mixed_precision}\")\n",
    "    logger.info(f\"  BetterTransformer: {config.use_better_transformer}\")\n",
    "    logger.info(f\"  Bucket Batching: {config.use_bucket_batching}\")\n",
    "    logger.info(f\"  Vectorized Postproc: {config.use_vectorized_postproc}\")\n",
    "    logger.info(f\"  Adaptive Beams: {config.use_adaptive_beams}\")\n",
    "\n",
    "    # Load test data\n",
    "    logger.info(f\"Loading test data from {config.test_data_path}\")\n",
    "    test_df = pd.read_csv(config.test_data_path, encoding=\"utf-8\")\n",
    "    logger.info(f\"‚úÖ Loaded {len(test_df)} test samples\")\n",
    "\n",
    "    # Print first samples\n",
    "    print(\"\\nFirst 5 samples:\")\n",
    "    print(test_df.head())\n",
    "\n",
    "    # Create engine\n",
    "    engine = UltraInferenceEngine(config, logger)\n",
    "\n",
    "    # Run inference\n",
    "    results_df = engine.run_inference(test_df)\n",
    "\n",
    "    # Save results and config\n",
    "    save_outputs(results_df, config, config.output_dir, logger)\n",
    "\n",
    "    # Optional inspection\n",
    "    inspect_results(config.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9147887,
     "sourceId": 14374989,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9181082,
     "sourceId": 14376272,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9203761,
     "sourceId": 14410665,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9366631,
     "sourceId": 14661932,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 222398,
     "modelInstanceId": 239470,
     "sourceId": 282751,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.099137,
   "end_time": "2026-02-03T12:17:17.813036",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-03T12:16:14.713899",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "260df5fb382f4860951c244d9908b4a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4026d5ab0de344c78e02a91aff597a60",
        "IPY_MODEL_a8b2b78579654412ad4d9e6c9ec438e7",
        "IPY_MODEL_26adb1aa4f184f27987adb617a8159ab"
       ],
       "layout": "IPY_MODEL_2f87086a580f46ddb6d074d1eec60cf1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "26adb1aa4f184f27987adb617a8159ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6681a986629242849f134b3878db45e4",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_dbafa77fa23a461eb025fa088dbfd278",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá4/4‚Äá[00:11&lt;00:00,‚Äá‚Äá2.77s/it]"
      }
     },
     "2f87086a580f46ddb6d074d1eec60cf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3366afbe69fa4eeb99d9ba38928b548d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4026d5ab0de344c78e02a91aff597a60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3366afbe69fa4eeb99d9ba38928b548d",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_e70cae0583bb4735a2a5e8250d9a982c",
       "tabbable": null,
       "tooltip": null,
       "value": "üöÄ‚ÄáTranslating‚Äá(byt5-akkadian-optimized-34x):‚Äá100%"
      }
     },
     "6681a986629242849f134b3878db45e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6fb833ad1c8642b0bc4706728f792e01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a8b2b78579654412ad4d9e6c9ec438e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fbf19f3b332b45ec92daae0a4f966b0d",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6fb833ad1c8642b0bc4706728f792e01",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "dbafa77fa23a461eb025fa088dbfd278": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e70cae0583bb4735a2a5e8250d9a982c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fbf19f3b332b45ec92daae0a4f966b0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
